{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling Using Distributed Word Embeddings\n",
    "================================================\n",
    "Notebook version of https://github.com/rsrandhawa/Vec2Topic code, based on the article \"Topic Modeling Using Distributed Word Embeddings\" by R. S. Randhawa, P. Jain, and G. Madan. \n",
    "\n",
    "The basic approach is to first create a language model based on a large (ideally billions of words) text corpus. The technology used, distributed word embeddings, is a shallow neural network that seems to perform best on large datasets (trades simple but fast computation for tons of data).\n",
    "\n",
    "The user generated content (which is usually a much smaller corpus) is likewise trained with consistent parameters. Vectors corresponding to the same vocabulary word are concatenated together to provide a model of the user generated content.\n",
    "\n",
    "Word vectors that cluster together are interperted as topics of the user generated content. Some clusters appear better than others because they consist of coherent lists of words -- main goal is to score the importance of each topic.\n",
    "\n",
    "Performing a hierarchical clustering provides a measure of depth for each word and computing a co-occurance graph (edge between two words if they belong to the same sentenence) provides a degree of co-occurance. Each word is scored by a (normalized) product of depth and degree. The awesome `hdbscan` is used to cluster words into topics, and the scoring function is used to order the words and the topics. Sample below with `min_cluster_size=5`.\n",
    "\n",
    "![Topics](topics.png)\n",
    "\n",
    "![Sentences](sentences.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Standard Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging, re, os, bz2, gzip, subprocess, uuid, json\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Unicode wrapper for reading & writing csv files.\n",
    "import unicodecsv as csv\n",
    "\n",
    "## Lighter weight than pandas -- tabular display of data.\n",
    "from tabulate import tabulate\n",
    "\n",
    "## In order to strip out the text from the xml formated data.\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Data Science Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First the usual suspects: numpy, pandas, scipy, and gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import gensim\n",
    "\n",
    "## For scraping text out of a wikipedia dump. Get dumps at https://dumps.wikimedia.org/backup-index.html\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "## Latest greatest word vectors (see https://pypi.python.org/pypi/fasttext).\n",
    "import fasttext\n",
    "\n",
    "## Package for segmenenting and finding parts-of-speech for Chinese.\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "## Latest greatest hierarchical clustering package. \n",
    "## Word vectors are clustered, with deeper trees indicating core topics.\n",
    "import hdbscan\n",
    "\n",
    "## Use scikit-learn to generate co-occurancy graph (edge if words in same sentence).\n",
    "## The degree of each word indicates how strong it co-occurs.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "#import bhtsne\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Data Directory and Logging\n",
    "\n",
    "The data directory, if it exists, should contain a copy of the background language model. Especially, the `data/` directory should contain the pre-computed word vectors for the wikipedia. If there is no language model, then set a flag for later processing.\n",
    "\n",
    "TODO: \n",
    "1. Implement more careful defaults for language models.\n",
    "2. Synchronize directory and filenames with those used in sumarrization (rouge) literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_directory = 'data/'\n",
    "model_directory = 'models/'\n",
    "eval_directory = 'eval/'\n",
    "wikipedia_vector_path = 'models_save/zhwiki-20160920-pages-articles.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "reload(logging)\n",
    "\n",
    "LOG_TO_FILE = False\n",
    "if LOG_TO_FILE:\n",
    "    LOG_FILENAME = data_directory + 'vec2topic.log'\n",
    "    logging.basicConfig(filename=LOG_FILENAME,level=logging.INFO)\n",
    "else:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s %(message)s',\"%b-%d-%Y %H:%M:%S\")\n",
    "logger.handlers[0].setFormatter(formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Language Model\n",
    "\n",
    "Typically pre-computed from wikipedia. If it does not exist, then proceed using the local (target) documents. The idea is that a background language model should capture common language ussage. Takes about 5 minutes to load.\n",
    "\n",
    "The `fasttext` package, which is used to precompute the wikipedia word vectors, produces two outputs. The set of all word vectors (file extension `.vec`) and the binary internal state of the neural network (file extension `.bin`). The algorithm below only uses the word vectors and so avoids the more expensive (in size and procesessing resources) binary model.\n",
    "\n",
    "TODO: \n",
    "1. Investigate using more compact format for word vectors (e.g. flat buffers or capt'n proto).\n",
    "2. Wider set of language models: parameters & domain specific background corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WikipediaWordVector_(namedtuple('WikipediaWordVector_', ('word','vector'))):\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, row):          \n",
    "        return cls(row[0],map(float,row[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class wikipedia_word_vectors(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self._length= 0\n",
    "        self._vector_dim = 0\n",
    "        self._wikipedia_vocab = []\n",
    "        \n",
    "        with open(self.path, 'rU') as data:\n",
    "            reader = csv.reader(data, delimiter=' ')\n",
    "            self._length, self._vector_dim = reader.next()\n",
    "            self._length = int(self._length)\n",
    "            self._vector_dim = int(self._vector_dim)\n",
    "            \n",
    "            self._wikipedia_vocab = {line.split()[0].decode('utf-8') for line in data}\n",
    "        \n",
    "        self._wikipedia_vocab = set(self._wikipedia_vocab)\n",
    "        print len(self._wikipedia_vocab), self._length\n",
    "\n",
    "    def __iter__(self, vocab=[]):\n",
    "        count = 0\n",
    "        oov = []\n",
    "        overlap = self._wikipedia_vocab.intersection(set(vocab))\n",
    "        for w in vocab:\n",
    "            if w not in overlap:\n",
    "                oov.append(w)\n",
    "        with open(self.path, 'rU') as data:\n",
    "            reader = csv.reader(data, delimiter=' ')\n",
    "            self._length, self._vector_dim = reader.next()\n",
    "            for row in reader:\n",
    "                if count == len(overlap):\n",
    "                    break\n",
    "                if row[0] in overlap:\n",
    "                    count += 1\n",
    "                    yield WikipediaWordVector_.parse(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LANGUAGE_MODEL = True\n",
    "## Check to see if working directories exist. If they don't, then create them.\n",
    "if not os.path.isdir(model_directory):\n",
    "    os.mkdir(model_directory)\n",
    "    LANGUAGE_MODEL = False\n",
    "    \n",
    "if not os.path.isfile(wikipedia_vector_path):\n",
    "    LANGUAGE_MODEL = False\n",
    "else:\n",
    "    wikipedia_reader = wikipedia_word_vectors(wikipedia_vector_path)\n",
    "\n",
    "if not os.path.isdir(data_directory):\n",
    "    os.mkdir(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_vocab = set([u'不滿', u'問題', 'zblah', u'工廠'])\n",
    "for r in wikipedia_reader.__iter__(test_vocab):\n",
    "    print r[0], len(r[1])\n",
    "    \n",
    "print wikipedia_reader.path, wikipedia_reader._length, wikipedia_reader._vector_dim\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rec = dict((f, getattr(r, f)) for f in r._fields)\n",
    "print rec['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare User Content\n",
    "\n",
    "Assumes that Stanford CoreNLP is running and listening on a socket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for dirpath, dirnames, filenames in os.walk('wikipedia_fa/zh/text/'):\n",
    "        for fn in filenames:\n",
    "            if '_body' in fn:\n",
    "                post_file = 'wikipedia_fa/zh/text/' + fn\n",
    "                out_file = post_file + '.json'\n",
    "                properties = 'localhost:9000/?properties={\"annotators\":\"tokenize,ssplit,pos\",\"outputFormat\":\"json\",\"pipelineLanguage\":\"zh\",\"ssplit.boundaryTokenRegex\":\"[.]|[!?]+|[。]|[！？]+\"}'\n",
    "                cmd = 'wget --post-file {} {} -O {} '.format(post_file, properties, out_file)\n",
    "                print cmd\n",
    "                cmd_list = cmd.split()\n",
    "                ret_val = subprocess.call(cmd_list)\n",
    "                print ret_val,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for dirpath, dirnames, filenames in os.walk('wikipedia_fa/zh/text/'):\n",
    "        for fn in filenames:\n",
    "            if '_summary' in fn:\n",
    "                post_file = 'wikipedia_fa/zh/text/' + fn\n",
    "                out_file = post_file + '.json'\n",
    "                properties = 'localhost:9000/?properties={\"annotators\":\"tokenize,ssplit,pos\",\"outputFormat\":\"json\",\"pipelineLanguage\":\"zh\",\"ssplit.boundaryTokenRegex\":\"[.]|[!?]+|[。]|[！？]+\"}'\n",
    "                cmd = 'wget --post-file {} {} -O {} '.format(post_file, properties, out_file)\n",
    "                print cmd\n",
    "                cmd_list = cmd.split()\n",
    "                ret_val = subprocess.call(cmd_list)\n",
    "                print ret_val,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class corenlp_text_data():\n",
    "    '''Assumes a directory full of json output generated by Standford CoreNLP.'''\n",
    "    def __init__(self, root_dir=None, file_names=None, index_name=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_names = file_names  ## An optional list of file names to scope analysis.\n",
    "        self.tmp_dir = 'tmp_dir/'\n",
    "        self.INDEX_NAME = 'test-v1'\n",
    "        self.dictionary = Dictionary()\n",
    "        \n",
    "        ## TODO: Instead of keeping multiple copies of data in memory, write generators to stream off of disk.\n",
    "        ## Keep data as unicode strings, then serialize dictionary as utf-8 json.\n",
    "        \n",
    "        ## Collect metadata on documents.\n",
    "        self.data_stats = {}\n",
    "        \n",
    "        ## One (full) sentence per line, input for skipgram.\n",
    "        self.text_sentences = []\n",
    "        \n",
    "        ## Format for mallet topic modeling. Input all text (actually nouns) all on one line.\n",
    "        ## <doc_id> <tab> <text_one_line>\n",
    "        self.mallet_input = []\n",
    "        \n",
    "        ## Restrict vocabulary to \"nouns\".\n",
    "        self.sentences_nouns = []\n",
    "        \n",
    "        ## For each noun, keep a mapping of all documents that it appears in.\n",
    "        self.document_noun = {}\n",
    "        self.document_topic = {}\n",
    "        \n",
    "        ## Temporary directory to store all the intermediate files.\n",
    "        if not os.path.isdir(self.tmp_dir):\n",
    "            os.mkdir(self.tmp_dir)\n",
    "            \n",
    "        ## Walk the input directory and process files.\n",
    "        ## Side effects: statistics on input directory\n",
    "        ##               temporary directory full of files formated for topic modeling\n",
    "        for dirpath, dirnames, filenames in os.walk(self.root_dir):\n",
    "            if self.file_names == None:\n",
    "                pass\n",
    "            else:\n",
    "                filenames = [fn for fn in filenames if fn in self.file_names]\n",
    "            for fn in filenames:\n",
    "                ## Assumption: json formatted output from Stanford corenlp -- sentences, word seg., pos.\n",
    "                if fn.endswith('.json'):\n",
    "\n",
    "                    ## Generate a document identifier, real document names can be ugly.\n",
    "                    key = str(uuid.uuid4())\n",
    "\n",
    "                    ## Pull out information encoded in directory name path.\n",
    "                    ## OpenSubtitles2016/raw/vi/2006/761212/3826993.xml.gz\n",
    "                    parts = dirpath.split('/')\n",
    "                    root = parts[0]\n",
    "                    subtitle_language = 'zh' #parts[2]\n",
    "                    subtitle_year = ' ' #parts[3]\n",
    "                    subtitle_id = ' ' #parts[4]\n",
    "                    \n",
    "                    ## Initialize statistics, add to his as each file is processed.\n",
    "                    self.data_stats[key] = {'_type':'document', '_index':self.INDEX_NAME, \n",
    "                                            'doc_id':key, 'subtitle_language':subtitle_language,\n",
    "                                            'subtitle_year':subtitle_year, 'subtitle_id':subtitle_id, \n",
    "                                            'filename':fn}\n",
    "                    \n",
    "                    ## Load and process segmented text.\n",
    "                    local_content_file = dirpath + '/' + fn\n",
    "                    self.process_local_content_file(fn=local_content_file, key=key)\n",
    "                    \n",
    "                    ## Load and process parts-of-speech.\n",
    "                    ##pos_file = local_content_file # + '.pos'\n",
    "                    ##self.process_parts_of_speech_file(fn=pos_file, key=key)\n",
    "                    \n",
    "        self.dictionary = Dictionary(self.get_texts())\n",
    "                \n",
    "    def process_local_content_file(self, fn=None, key=None):\n",
    "        ## Keep track of lines (actual sentences because of preprocessing) infile.\n",
    "        text_lines_file = []\n",
    "        sentences_nouns = []\n",
    "                    \n",
    "        with open(fn,'rb') as fp:\n",
    "            data = json.load(fp)\n",
    "            for s in data['sentences']:\n",
    "                ## u'tokens', u'word', u'pos'\n",
    "                ## print s['index'], s.keys()\n",
    "                ## Type line is unicode.\n",
    "                line = ' '.join([w['word'] for w in s['tokens']])\n",
    "                text_lines_file.append(line)\n",
    "                \n",
    "                ## NR (proper noun), NT (temporal noun), NN (other noun)\n",
    "                ## Strip out just the nouns from the current sentence out of file fn.\n",
    "                ## Type of sentence_nouns: list of unicode strings (multi-character nouns).\n",
    "                sentence_nouns = [w['word'] for w in s['tokens'] if w['pos'] in ['NR','NT','NN']]\n",
    "                \n",
    "                ## Keep track of one long list of all sentences in all files.\n",
    "                sentences_nouns.append(' '.join(sentence_nouns))\n",
    "              \n",
    "        ## Save sentences (just the nouns) to temporary file.\n",
    "        with open(self.tmp_dir + key + '.txt','w') as fp:\n",
    "            for s in sentences_nouns:\n",
    "                ## Sentence s has type unicode (sequence of code points). \n",
    "                ## For output, encode it as a sequence of utf-8 bytes.\n",
    "                fp.write(s.encode('utf-8') + '\\n')\n",
    "\n",
    "        ## Find the set of all unique nouns identified in the current file.\n",
    "        ## Type of text_vocab_nouns is a set of unicode strings (multi-character nouns).\n",
    "        nouns = [w for s in sentences_nouns for w in s.split()]\n",
    "        text_vocab_nouns = set(nouns)\n",
    "\n",
    "        ## Keep the global array text_sentences, which is a list of lists. \n",
    "        self.text_sentences.append(text_lines_file)\n",
    "        \n",
    "        ## Throw all the lines (sentences) from the current file into one long line.\n",
    "        text_all_one_line = ' '.join(text_lines_file)\n",
    "        \n",
    "        ## Complete vocabulary for current file. List of unicode strings (multi-character words).\n",
    "        text_vocab = set([w for text in text_lines_file for w in text.split()])\n",
    "        \n",
    "        ## Keep track of files that the nouns came from.\n",
    "        for w in text_vocab_nouns:\n",
    "            if w in self.document_noun:\n",
    "                self.document_noun[w].append(key)\n",
    "            else:\n",
    "                self.document_noun[w] = [key]\n",
    "                \n",
    "        ## Throw all the nouns of the document into one unicode string. \n",
    "        ## This will be a field stored in elasticsearch.\n",
    "        text_vocab_nouns_oneline = ' '.join(list(text_vocab_nouns))\n",
    "        \n",
    "        self.sentences_nouns.append(sentences_nouns)\n",
    "        self.mallet_input.append((key,text_all_one_line))\n",
    "        \n",
    "        self.data_stats[key]['text'] = text_all_one_line\n",
    "        self.data_stats[key]['no_sents'] = len(text_lines_file)\n",
    "        self.data_stats[key]['no_words'] = len(text_vocab)     \n",
    "        self.data_stats[key]['word'] = list(text_vocab_nouns)\n",
    "        self.data_stats[key]['no_nouns'] = len(text_vocab_nouns)\n",
    "\n",
    "    def load_segmented_sentences(self):\n",
    "        for key in self.data_stats:\n",
    "            yield self.data_stats[key]\n",
    "            \n",
    "    def __iter__(self):\n",
    "        for line in open('test.txt'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())\n",
    "            \n",
    "    def get_texts(self):\n",
    "        sentences_nouns = [s.split() for f in self.sentences_nouns for s in f]\n",
    "        for line in sentences_nouns:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cjklib.dictionary import CEDICT\n",
    "d = CEDICT()\n",
    "\n",
    "for t in d.getForHeadword(u'生存'):\n",
    "    print t.HeadwordSimplified, t.Reading, t.Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_word_vectors(text_fn='tmp.txt', vector_dim=25, language_model=LANGUAGE_MODEL, sentences_nouns=None):\n",
    "    ## Three basic steps:\n",
    "    ## 1) Compute word embeddings of local text file.\n",
    "    ## 2) Find overlap between local vocab and backgrond language_model base (e.g. language specific wikipedia).\n",
    "    ## 3) Concatenate local and language_model base vectors.\n",
    "    \n",
    "    ## TODO: \n",
    "    ## Reconsider logic to recompute the vectors. Support running many experiments on same set of vectors.\n",
    "    ## Clean up local variable names.\n",
    "    ## Don't pass in sentences_nouns\n",
    "\n",
    "    logger.info('Calculating skipgram vectors.')\n",
    "    local_content_skipgram = fasttext.skipgram(text_fn, model_directory + 'tmp', \n",
    "        lr=0.02, dim=vector_dim, ws=5,\n",
    "        epoch=5, min_count=0, neg=5, loss='ns', bucket=2000000, minn=1, maxn=4,\n",
    "        thread=8, t=1e-5, lr_update_rate=100)\n",
    "        \n",
    "    words = [w for text in sentences_nouns for w in text]\n",
    "    #nouns = [n for f in nouns_all for n in f]\n",
    "    Vocab = set(words)\n",
    "\n",
    "    if language_model == False:\n",
    "        model_comb = local_content_skipgram\n",
    "        model_comb_vocab = list(Vocab)\n",
    "        return model_comb, model_comb_vocab\n",
    "    else:\n",
    "        logger.info('Concatenate local and language_model base word vectors: dim = %d', \n",
    "                    int(wikipedia_reader._vector_dim) + vector_dim)\n",
    "        \n",
    "        model_comb={}\n",
    "        model_comb_vocab=[]\n",
    "        \n",
    "        for r in wikipedia_reader.__iter__(Vocab):\n",
    "            record = dict((f, getattr(r, f)) for f in r._fields)\n",
    "            w = record['word']\n",
    "            v = np.array(record['vector'])\n",
    "            model_comb[w] = np.array(np.concatenate((v,local_content_skipgram[w])))\n",
    "            model_comb_vocab.append(w)\n",
    "\n",
    "        logger.info('Length of common_vocab: %d', len(model_comb_vocab))\n",
    "\n",
    "        combined_vectors = model_directory + 'tmp.combined_vectors.txt'\n",
    "        writer = csv.writer(open(combined_vectors,'w'),delimiter='\\t')\n",
    "        for k in model_comb.keys():\n",
    "            writer.writerow(model_comb[k])\n",
    "\n",
    "        return model_comb, model_comb_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Helper Functions\n",
    "def norm(a):\n",
    "    return np.sqrt(np.sum(np.square(a)))\n",
    "\n",
    "def cosine(a,b):\n",
    "    return 1-np.dot(a,b)/np.sqrt(np.sum(a**2)*np.sum(b**2))\n",
    "\n",
    "def l1(a,b):\n",
    "    return abs(a-b).sum()\n",
    "\n",
    "def l2(a,b):\n",
    "    return np.sqrt(np.square(a-b).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score vocabulary for depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create a list of words to be clustered based on a model with some l2_threshold and can normalize the vectors \n",
    "### and also repeat or no\n",
    "def create_word_list(model=None, vocab=None, features=None, Texts=None, repeat=True,\n",
    "                     l2_threshold=0, normalized=True, min_count=100, min_length=0):\n",
    "    data_d2v=[]\n",
    "    word_d2v=[]\n",
    "    words_text=[w for text in Texts for w in text]\n",
    "    count=Counter(words_text)\n",
    "\n",
    "    A=set(words_text)\n",
    "    for w in vocab:\n",
    "        if w in A and len(w)>min_length and l2(model[w],np.zeros(features))>l2_threshold and count[w]>min_count:\n",
    "            if normalized:\n",
    "                data_d2v.append(model[w]/l2(model[w],np.zeros(features)))\n",
    "            else:\n",
    "                data_d2v.append(model[w])\n",
    "            word_d2v.append(w)\n",
    "        else:\n",
    "            logger.info('Mismatch: ', w)\n",
    "\n",
    "    return data_d2v, word_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_vectors(data_d2v):\n",
    "    min_cluster_size_opt = 0\n",
    "    min_samples_opt = 0\n",
    "    count = 0\n",
    "    while True:\n",
    "        tsne = TSNE(n_jobs=4)\n",
    "        X_2D = tsne.fit_transform(np.array(data_d2v))\n",
    "        logger.info('Attempt: %d', count)\n",
    "        count += 1\n",
    "        cluster_params = []\n",
    "        label_values = []\n",
    "        for min_cluster_size in range(4,30):\n",
    "            for min_samples in range(4,min_cluster_size):\n",
    "                clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "                #labels = clusterer.fit_predict(np.array(data_d2v))\n",
    "                labels = clusterer.fit_predict(X_2D)\n",
    "                label_max = clusterer.labels_.max()\n",
    "                if label_max >= 20:\n",
    "                    label_values.append(label_max)\n",
    "                    cluster_params.append((min_cluster_size,min_samples))\n",
    "                    logger.info('min_samples:%d, min_cluster_size:%d, label_max:%d',\n",
    "                                    min_samples,min_cluster_size,label_max)\n",
    "        label_values.reverse()\n",
    "        cluster_params.reverse()\n",
    "        if len(label_values) > 0:\n",
    "            i = np.argmax(label_values)\n",
    "            min_cluster_size_opt, min_samples_opt = cluster_params[i]\n",
    "            break\n",
    "    \n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size_opt, min_samples=min_samples_opt) \n",
    "    return X_2D, clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_depth(clusterer,words, num_points):\n",
    "    cluster=[[] for w in xrange(2*num_points)]\n",
    "    c=Counter()\n",
    "    for i in xrange(num_points):\n",
    "        cluster[i]=[i]\n",
    "\n",
    "    for i in xrange(len(clusterer)):\n",
    "        x=int(clusterer[i,0])\n",
    "        y=int(clusterer[i,1])\n",
    "        xval=[w for w in cluster[x]]\n",
    "        yval=[w for w in cluster[y]]\n",
    "        cluster[num_points+i]=xval+yval\n",
    "        for w in cluster[num_points+i]:\n",
    "            c[words[w]]+=1\n",
    "        cluster[x][:]=[]\n",
    "        cluster[y][:]=[]    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score vocabulary for co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_occurence_graph(sentences_nouns=None, combined_vocab=None):\n",
    "    logger.info('Computing co-occurence graph')\n",
    "\n",
    "    T=[' '.join(w) for w in sentences_nouns]\n",
    "    \n",
    "    ##Co-occurence matrix\n",
    "    #cv=CountVectorizer(token_pattern=u'(?u)\\\\b([^\\\\s]+)')\n",
    "    cv=CountVectorizer(vocabulary=combined_vocab)\n",
    "    bow_matrix = cv.fit_transform(T)\n",
    "    id2word={}\n",
    "    for key, value in cv.vocabulary_.items():\n",
    "        id2word[value]=key\n",
    "\n",
    "    ids=[]\n",
    "    for key,value in cv.vocabulary_.iteritems():\n",
    "        if key in combined_vocab:\n",
    "            ids.append(value)\n",
    "\n",
    "    sort_ids=sorted(ids)\n",
    "    bow_reduced=bow_matrix[:,sort_ids]\n",
    "    normalized = TfidfTransformer().fit_transform(bow_reduced)\n",
    "    similarity_graph_reduced=bow_reduced.T * bow_reduced\n",
    "    \n",
    "    ##Depth-rank weighting of edges, weight of edge i,j=cosine of angle between them\n",
    "    logger.info('Computing degree')\n",
    "    m,n=similarity_graph_reduced.shape\n",
    "\n",
    "    cx=similarity_graph_reduced.tocoo()\n",
    "    keyz=[id2word[sort_ids[w]] for w in xrange(len(sort_ids))]\n",
    "    data=[]\n",
    "    ro=[]\n",
    "    co=[]\n",
    "    for i,j,v in itertools.izip(cx.row, cx.col, cx.data):\n",
    "        if v>0 and i!=j:\n",
    "            value=1\n",
    "            if value>0:\n",
    "                ro.append(i)\n",
    "                co.append(j)\n",
    "                data.append(value)\n",
    "\n",
    "    SS=sp.sparse.coo_matrix((data, (ro, co)), shape=(m,n))\n",
    "    SP_full=SS.tocsc()\n",
    "    id_word={w:id2word[sort_ids[w]] for w in xrange(len(sort_ids))}\n",
    "    word_id={value:key for key,value in id_word.items()}\n",
    "    degsum=SP_full.sum(axis=1)\n",
    "    deg={}\n",
    "    for x in xrange(len(sort_ids)):\n",
    "        deg[id2word[sort_ids[x]]]=int(degsum[x])\n",
    "    \n",
    "    return deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summarize_document(file_name=None):\n",
    "    root_dir = 'wikipedia_fa/zh/text/'\n",
    "    vector_dim = 25\n",
    "    \n",
    "    sample_data = corenlp_text_data(root_dir=root_dir,file_names=[file_name])\n",
    "    stats = [sample_data.data_stats[key] for key in sample_data.data_stats]\n",
    "    df = pd.DataFrame(stats)\n",
    "    sentences_nouns = [s.split() for f in sample_data.sentences_nouns for s in f]\n",
    "    \n",
    "    with open('tmp.txt','w') as fp:\n",
    "        for s in sentences_nouns:\n",
    "            s_string = ' '.join(s)\n",
    "            fp.write(s_string.encode('utf-8') + '\\n')\n",
    "            \n",
    "    model_comb, model_comb_vocab = compute_word_vectors(language_model=LANGUAGE_MODEL, vector_dim=vector_dim,\n",
    "                                                        sentences_nouns=sentences_nouns)\n",
    "\n",
    "    logger.info('Clustering for depth...')\n",
    "    local_vec = True\n",
    "\n",
    "    features = int(wikipedia_reader._vector_dim)*LANGUAGE_MODEL + vector_dim\n",
    "    data_d2v,word_d2v = create_word_list(model=model_comb, vocab=model_comb_vocab, features=features, \n",
    "                                         Texts=sentences_nouns, l2_threshold=0, normalized=True, \n",
    "                                         min_count=0, min_length=0)\n",
    "\n",
    "    X_2D, clusterer = cluster_vectors(data_d2v)\n",
    "    labels = clusterer.fit_predict(X_2D)\n",
    "    \n",
    "    mcv = {}\n",
    "    for n,w in enumerate(word_d2v):\n",
    "        mcv[w] = n\n",
    "    ##Calculate depth of words\n",
    "    depth = calculate_depth(clusterer.single_linkage_tree_.to_numpy(), word_d2v, len(data_d2v))\n",
    "    deg = co_occurence_graph(sentences_nouns=sentences_nouns, combined_vocab=mcv)\n",
    "    \n",
    "    logger.info('Computing metrics')\n",
    "\n",
    "    max_deg=max(deg.values())\n",
    "    max_depth=max(depth.values())\n",
    "\n",
    "    temp_deg_mod={w:np.log(1+deg[w])/np.log(1+max_deg) for w in deg.iterkeys()}\n",
    "    alpha=np.log(0.5)/np.log(np.median(temp_deg_mod.values()))\n",
    "    deg_mod={key:value**alpha for key, value in temp_deg_mod.iteritems()}\n",
    "\n",
    "    temp={key:value*1./max_depth for key, value in depth.iteritems()}\n",
    "    alpha=np.log(0.5)/np.log(np.median(temp.values()))\n",
    "    depth_mod={key:value**alpha for key, value in temp.iteritems()}\n",
    "\n",
    "    temp={key:deg_mod[key]*depth_mod[key] for key in depth_mod.iterkeys()}\n",
    "    max_metric=np.max(temp.values())\n",
    "    metric={key:value*1./max_metric for key,value in temp.iteritems()}\n",
    "\n",
    "    logger.info('max_deg = %s, max_depth = %s',max_deg, max_depth)\n",
    "    \n",
    "    K = clusterer.labels_.max()+1\n",
    "    print K\n",
    "    cluster_label={word_d2v[x]:labels[x] for x in xrange(len(word_d2v))}\n",
    "\n",
    "    cluster_label_ranked={}\n",
    "\n",
    "    topic=[[] for i in xrange(-1,K)]\n",
    "    clust_depth=[[] for i in xrange(K)]\n",
    "    for i in xrange(K):\n",
    "        topic[i]=[word_d2v[x] for x in xrange(len(word_d2v)) if labels[x]==i]\n",
    "        #temp_score=[metric[w] for w in topic[i]]\n",
    "        temp_score = []\n",
    "        for w in topic[i]:\n",
    "            if w in metric: temp_score.append(metric[w])\n",
    "        clust_depth[i]=-np.sum(sorted(temp_score,reverse=True)[:])#int(np.sqrt(len(topic[i])))])\n",
    "    index=np.argsort(clust_depth)\n",
    "    index2=np.argsort(-clusterer.cluster_persistence_)\n",
    "    for i in xrange(K):\n",
    "        for w in topic[index[i]]:\n",
    "            cluster_label_ranked[w]=i\n",
    "\n",
    "    noise = [word_d2v[x] for x in xrange(len(word_d2v)) if labels[x]==-1]\n",
    "    for w in noise:\n",
    "        cluster_label_ranked[w] = -1\n",
    "        \n",
    "    logger.info('Done...Generating output')\n",
    "    lister=[]\n",
    "    to_show=K\n",
    "    to_show_words=200 #the maximum number of words of each type to display\n",
    "    for i in xrange(to_show):\n",
    "        top=topic[index[i]]\n",
    "        sort_top=[w[0] for w in sorted([[w,metric[w]] for w in top],key=itemgetter(1),reverse=True)]\n",
    "        lister.append(['Topic %d' %(i+1)]+sort_top[:to_show_words])\n",
    "\n",
    "    max_len=max([len(w) for w in lister])\n",
    "    new_list=[]\n",
    "    for list_el in lister:\n",
    "        new_list.append(list_el + [''] * (max_len - len(list_el)))\n",
    "    Topics=list(itertools.izip_longest(*new_list))\n",
    "    #X.insert(len(X),[-int(clust_depth[index[w]]*100)*1./100 for w in xrange(K)])\n",
    "    sorted_words=[w[0] for w in sorted(metric.items(),key=itemgetter(1),reverse=True)][:to_show_words]\n",
    "\n",
    "    return sample_data, word_d2v, metric, Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fn = 'addd9d361106ea4c7a79e3a79c8be18d_body.txt.json'\n",
    "fn = '89e8cfc72f7fde2851cd174dc73b2c44_body.txt.json'\n",
    "sample_data, word_d2v, metric, Topics = summarize_document(file_name=fn)\n",
    "df = pd.DataFrame([sample_data.data_stats[key] for key in sample_data.data_stats])\n",
    "df[['subtitle_language','subtitle_year','subtitle_id','filename', 'no_sents','no_words','no_nouns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_topics = pd.DataFrame([Topics[i][0:15] for i in range(0,11)])\n",
    "df_topics.columns = df_topics.iloc[0]\n",
    "df_topics = df_topics.reindex(df_topics.index.drop(0))\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "lines = LineSentence('tmp.txt')\n",
    "dictionary = corpora.Dictionary(lines)\n",
    "sentences_nouns = [s.split() for f in sample_data.sentences_nouns for s in f]\n",
    "with open('lengths.txt','w') as fp:\n",
    "    for sentence_id, sentence in enumerate(sentences_nouns):\n",
    "        fp.write(str(len(sentence)) + '\\n')\n",
    "\n",
    "with open('sets.txt','w') as fp:\n",
    "    for sentence_id, sentence in enumerate(sentences_nouns):\n",
    "        #print len(sentence), sentence\n",
    "        term_ids = [w[0] for w in dictionary.doc2bow(sentence)]\n",
    "        for term_id in term_ids:\n",
    "            fp.write(str(sentence_id+1) + ' ' + str(term_id+1) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(dictionary), dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('vocab.txt','w') as fp:\n",
    "    for n in range(0,len(dictionary)):\n",
    "        w = dictionary.get(n)\n",
    "        try:\n",
    "            fp.write(str(metric[w])+'\\n')\n",
    "        except:\n",
    "            fp.write(str(0) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nz = 3700\n",
    "M = len(dictionary)\n",
    "N = len(sentences_nouns)\n",
    "s = 100\n",
    "D = 'sets.txt'\n",
    "W = 'vocab.txt'\n",
    "L = 'lengths.txt'\n",
    "b = 5\n",
    "cmd = '/root/notebooks/stage/mad-science/OCCAMSV5/occams_v5 -z {} -m {} -n {} -s {} -D {} -W {} -L {} -b {}'.format(nz, M, N, s, D, W, L, b)\n",
    "print cmd\n",
    "cmd_list = cmd.split()\n",
    "ret_val = subprocess.check_output(cmd_list, stderr=subprocess.STDOUT)\n",
    "for line in ret_val.split('\\n'):\n",
    "    if 'Chosen sentences' in line:\n",
    "        print line\n",
    "        sentence_ids = line.split(':')[1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_nouns = []\n",
    "for n in [int(n)-1 for n in sentence_ids]:\n",
    "    summary_nouns.append(' '.join(sample_data.sentences_nouns[0][n].split()))\n",
    "    #print sample_data.sentences_nouns[0][n].split()\n",
    "    #print n, '|'.join(sample_data.sentences_nouns[0][n].split())\n",
    "\n",
    "summary_nouns = set([w for line in summary_nouns for w in line.split()])\n",
    "readings = []\n",
    "translations = []\n",
    "for w in summary_nouns:\n",
    "    try:\n",
    "        t = d.getForHeadword(w).next()\n",
    "        readings.append(t.Reading)\n",
    "        translations.append(t.Translation)\n",
    "    except:\n",
    "        readings.append(' ')\n",
    "        translations.append(' ')\n",
    "        \n",
    "topic_numbers = []\n",
    "for w in summary_nouns:\n",
    "    try:\n",
    "        topic_numbers.append(cluster_label_ranked[w]+1)\n",
    "    except:\n",
    "        topic_numbers.append(-1)\n",
    "        \n",
    "weights = []\n",
    "for w in summary_nouns:\n",
    "    try:\n",
    "        weights.append(metric[w])\n",
    "    except:\n",
    "        weights.append(-1)\n",
    "terms_df = pd.DataFrame(zip(summary_nouns,readings,translations,weights,topic_numbers),\n",
    "                        columns=['Noun','Reading','Translation','Weight','Topic'])\n",
    "print len(terms_df)\n",
    "grouped = terms_df.groupby('Topic')\n",
    "for name,group in grouped:\n",
    "    print name, #group.sum(),\n",
    "#    print group\n",
    "terms_df.sort_values(by='Weight',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sids = [int(sid)-1 for sid in sentence_ids]\n",
    "with open('summary.txt','w') as fp:\n",
    "    for sid in sids:\n",
    "        s = sample_data.text_sentences[0][sid]\n",
    "        fp.write(s.encode('utf-8') + '\\n')\n",
    "        print str(sid)+': ', sample_data.text_sentences[0][sid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_eval_info(filename=None,exp_no=1):\n",
    "    ## System summaries (i.e. output from algorithm).\n",
    "    fn = filename.split('.')[0]\n",
    "    fn_base = re.sub('_body','', fn)\n",
    "    system_vocab = []\n",
    "    out_fn = 'eval/systems/' + fn_base + '.' + str(exp_no) + '.txt'\n",
    "    print out_fn\n",
    "    with open(out_fn,'w') as fp:\n",
    "        for sid in sids:\n",
    "            s = sample_data.text_sentences[0][sid]\n",
    "            system_vocab.extend(s.split())\n",
    "            fp.write(s.encode('utf-8') + '\\n')\n",
    "    system_vocab = set(system_vocab)\n",
    "    print len(system_vocab)\n",
    "    for w in system_vocab:\n",
    "        print w,\n",
    "            \n",
    "    ## Model summaries (i.e. gold standard).\n",
    "    root_dir = 'wikipedia_fa/zh/text/'\n",
    "    fn_summary = re.sub('_body','_summary', fn)\n",
    "    model_vocab = []\n",
    "    !ls {root_dir + fn_summary + '.txt.json'}\n",
    "    data = corenlp_text_data(root_dir=root_dir,file_names=[fn_summary + '.txt.json'])\n",
    "    sentences_nouns = [s.split() for f in data.sentences_nouns for s in f]\n",
    "    out_fn = 'eval/models/' + fn_base + '.A.' + str(exp_no) + '.txt'\n",
    "    print out_fn\n",
    "    with open(out_fn,'w') as fp:\n",
    "        for s in sentences_nouns:\n",
    "            model_vocab.extend(s)\n",
    "            s_string = ' '.join(s)\n",
    "            fp.write(s_string.encode('utf-8') + '\\n')\n",
    "    model_vocab = set(model_vocab)\n",
    "    print len(model_vocab)\n",
    "    for w in model_vocab:\n",
    "        print w,\n",
    "    \n",
    "    print '\\n' + '=='*40\n",
    "    for w in model_vocab.intersection(system_vocab):\n",
    "        print w,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_eval_info_nouns(filename=None,exp_no=1):\n",
    "    ## System summaries (i.e. output from algorithm).\n",
    "    fn = filename.split('.')[0]\n",
    "    fn_base = re.sub('_body','', fn)\n",
    "    system_vocab = []\n",
    "    out_fn = 'eval/systems/' + fn_base + '.' + str(exp_no) + '.txt'\n",
    "    print out_fn\n",
    "    with open(out_fn,'w') as fp:\n",
    "        for sid in sids:\n",
    "            s = sample_data.sentences_nouns[0][sid]\n",
    "            system_vocab.extend(s.split())\n",
    "            fp.write(s.encode('utf-8') + '\\n')\n",
    "    s_peer = ' '.join(system_vocab)\n",
    "    #s_peer = ' '.join(list(s_peer)) ## Look at just unicode characters instead of words.\n",
    "    print type(s_peer)\n",
    "    system_vocab = set(system_vocab)\n",
    "    #print len(system_vocab)\n",
    "    #for w in system_vocab:\n",
    "    #    print w,\n",
    "            \n",
    "    ## Model summaries (i.e. gold standard).\n",
    "    root_dir = 'wikipedia_fa/zh/text/'\n",
    "    fn_summary = re.sub('_body','_summary', fn)\n",
    "    model_vocab = []\n",
    "    !ls {root_dir + fn_summary + '.txt.json'}\n",
    "    data = corenlp_text_data(root_dir=root_dir,file_names=[fn_summary + '.txt.json'])\n",
    "    sentences_nouns = [s.split() for f in data.sentences_nouns for s in f]\n",
    "    out_fn = 'eval/models/' + fn_base + '.A.' + str(exp_no) + '.txt'\n",
    "    print out_fn\n",
    "    with open(out_fn,'w') as fp:\n",
    "        for s in sentences_nouns:\n",
    "            model_vocab.extend(s)\n",
    "            s_string = ' '.join(s)\n",
    "            fp.write(s_string.encode('utf-8') + '\\n')\n",
    "    s_model = ' '.join(model_vocab)\n",
    "    #s_model = ' '.join(list(s_model)) ## Look at just unicode characters instead of words.\n",
    "    model_vocab = set(model_vocab)\n",
    "    #print len(model_vocab)\n",
    "    #for w in model_vocab:\n",
    "    #    print w,\n",
    "    \n",
    "    #print '\\n' + '=='*40\n",
    "    #for w in model_vocab.intersection(system_vocab):\n",
    "    #    print w,\n",
    "        \n",
    "    return s_peer,s_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_peer,s_model = write_eval_info_nouns(df['filename'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import tee, islice\n",
    "\n",
    "def ngrams(lst, n):\n",
    "  tlst = lst\n",
    "  while True:\n",
    "    a, b = tee(tlst)\n",
    "    l = tuple(islice(a, n))\n",
    "    if len(l) == n:\n",
    "      yield l\n",
    "      next(b)\n",
    "      tlst = b\n",
    "    else:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rouge_n_r(s_peer=None,s_model=None,ngram_n=1,display=False):\n",
    "    s_peer_ngrams = Counter([ng for ng in ngrams(s_peer.split(),ngram_n)])\n",
    "    s_model_ngrams = Counter([ng for ng in ngrams(s_model.split(),ngram_n)])\n",
    "\n",
    "    t = 0\n",
    "    tab = [['word','peer cnt','model cnt','min']]\n",
    "    for ng in s_peer_ngrams:\n",
    "        if ng in s_model_ngrams:\n",
    "            t += min(s_peer_ngrams[ng],s_model_ngrams[ng]) ## Clip\n",
    "            tab.append([' '.join(ng),s_peer_ngrams[ng],s_model_ngrams[ng],min(s_peer_ngrams[ng],s_model_ngrams[ng])])\n",
    "            \n",
    "    rnr = float(t)/sum(s_model_ngrams.values())\n",
    "\n",
    "    if display == True:\n",
    "        print 'total ' + str(ngram_n) + '-gram model count: ', sum(s_model_ngrams.values())\n",
    "        print 'total ' + str(ngram_n) + '-gram peer count: ', sum(s_peer_ngrams.values())\n",
    "        print 'total ' + str(ngram_n) + '-gram hit: ', t\n",
    "        print 'total ROUGE-' + str(ngram_n) + '-R: ', float(t)/sum(s_model_ngrams.values())\n",
    "        print tabulate(tab,headers='firstrow',tablefmt='psql')\n",
    "        \n",
    "    return rnr,tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngram_n = 1\n",
    "rnr,tab = rouge_n_r(s_peer,s_model,ngram_n,display=False)\n",
    "rouge_df = pd.DataFrame(tab)\n",
    "rouge_df.columns = rouge_df.iloc[0]\n",
    "rouge_df = rouge_df.reindex(rouge_df.index.drop(0))\n",
    "print 'total ROUGE-' + str(ngram_n) + '-R: ', rnr\n",
    "rouge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngram_n = 2\n",
    "rnr,tab = rouge_n_r(s_peer,s_model,ngram_n,display=False)\n",
    "rouge_df = pd.DataFrame(tab)\n",
    "rouge_df.columns = rouge_df.iloc[0]\n",
    "rouge_df = rouge_df.reindex(rouge_df.index.drop(0))\n",
    "print 'total ROUGE-' + str(ngram_n) + '-R: ', rnr\n",
    "rouge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngram_n = 3\n",
    "rnr,tab = rouge_n_r(s_peer,s_model,ngram_n,display=False)\n",
    "rouge_df = pd.DataFrame(tab)\n",
    "rouge_df.columns = rouge_df.iloc[0]\n",
    "rouge_df = rouge_df.reindex(rouge_df.index.drop(0))\n",
    "print 'total ROUGE-' + str(ngram_n) + '-R: ', rnr\n",
    "rouge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: This does not match output from ROUGE perl script.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rouge_skipgram_s2(s_peer=None,s_model=None,max_skip=2,display=False):\n",
    "    tokens = ' '.join(list(s_peer)).split()  ## Characters (UNICODE)\n",
    "    #tokens = s_peer.split()                 ## Words (From preprocessing segmentation step.)\n",
    "    sgrams = [(tokens[index], tokens[index+j]) for index in range(len(tokens)) for j in range(1,max_skip+1) if (index + j) < len(tokens)]\n",
    "    s_peer_ngrams = Counter([ng for ng in sgrams])\n",
    "    \n",
    "    tokens = ' '.join(list(s_model)).split()\n",
    "    #tokens = s_model.split()\n",
    "    sgrams = [(tokens[index], tokens[index+j]) for index in range(len(tokens)) for j in range(1,max_skip+1) if (index + j) < len(tokens)]\n",
    "    s_model_ngrams = Counter([ng for ng in sgrams])\n",
    "\n",
    "    t = 0\n",
    "    tab = [['word','peer cnt','model cnt','min']]\n",
    "    for ng in s_peer_ngrams:\n",
    "        if ng in s_model_ngrams:\n",
    "            t += min(s_peer_ngrams[ng],s_model_ngrams[ng]) ## Clip\n",
    "            tab.append([' '.join(ng),s_peer_ngrams[ng],s_model_ngrams[ng],min(s_peer_ngrams[ng],s_model_ngrams[ng])])\n",
    "            \n",
    "    rnr = float(t)/sum(s_model_ngrams.values())\n",
    "\n",
    "    if display == True:\n",
    "        print 'total ROUGE-S' + str(max_skip) + ' model count: ', sum(s_model_ngrams.values())\n",
    "        print 'total ROUGE-S' + str(max_skip) + ' peer count: ', sum(s_peer_ngrams.values())\n",
    "        print 'total ROUGE-S' + str(max_skip) + ' hit: ', t\n",
    "        print 'total ROUGE-S' + str(max_skip) + '-R: ', float(t)/sum(s_model_ngrams.values())\n",
    "        print tabulate(tab,headers='firstrow',tablefmt='psql')\n",
    "        \n",
    "    return rnr,tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rnr,tab = rouge_skipgram_s2(s_peer,s_model,display=False)\n",
    "rouge_df = pd.DataFrame(tab)\n",
    "rouge_df.columns = rouge_df.iloc[0]\n",
    "rouge_df = rouge_df.reindex(rouge_df.index.drop(0))\n",
    "print 'total ROUGE-' + str(ngram_n) + '-R: ', rnr\n",
    "rouge_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
