{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling Using Distributed Word Embeddings\n",
    "================================================\n",
    "Notebook version of https://github.com/rsrandhawa/Vec2Topic code, based on the article \"Topic Modeling Using Distributed Word Embeddings\" by R. S. Randhawa, P. Jain, and G. Madan. \n",
    "\n",
    "The basic approach is to first create a language model based on a large (ideally billions of words) text corpus. The technology used, distributed word embeddings, is a shallow neural network that seems to perform best on large datasets (trades simple but fast computation for tons of data).\n",
    "\n",
    "The user generated content (which is usually a much smaller corpus) is likewise trained with consistent parameters. Vectors corresponding to the same vocabulary word are concatenated together to provide a model of the user generated content.\n",
    "\n",
    "Word vectors that cluster together are interperted as topics of the user generated content. Some clusters appear better than others because they consist of coherent lists of words -- main goal is to score the importance of each topic.\n",
    "\n",
    "Performing a hierarchical clustering provides a measure of depth for each word and computing a co-occurance graph (edge between two words if they belong to the same sentenence) provides a degree of co-occurance. Each word is scored by a (normalized) product of depth and degree. KMeans is used to cluster words into topics, and the scoring function is used to order the words and the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required standard packages\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging, re, os, bz2\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Unicode wrapper for reading & writing csv files.\n",
    "import unicodecsv as csv\n",
    "\n",
    "## Lighter weight than pandas -- tabular display of tables.\n",
    "from tabulate import tabulate\n",
    "\n",
    "## In order to strip out the text from the xml formated data.\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required data science packages\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First the usual suspects: numpy, scipy, and gensim\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gensim\n",
    "\n",
    "## For scraping text out of a wikipedia dump. Get dumps at https://dumps.wikimedia.org/backup-index.html\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "## Latest greatest word vectors (see https://pypi.python.org/pypi/fasttext).\n",
    "import fasttext\n",
    "\n",
    "## Latest greatest hierarchical clustering package. \n",
    "## Word vectors are clustered, with deeper trees indicating core topics.\n",
    "import fastcluster\n",
    "\n",
    "## Use scikit-learn to generate co-occurancy graph (edge if words in same sentence).\n",
    "## The degree of each word indicates how strong it co-occurs.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "## Use scikit-learn for K-Means clustering: identify topics.\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base data directory and logging.\n",
    "--------------------------------\n",
    "The approach currently uses a lot of intermediate files (which is annoying, but means that the project can work on machines with smaller physical memory). The initial data (knowledge base as well as user generated content) and the intermediate files are all kept in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_directory = 'data/'\n",
    "model_directory = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "reload(logging)\n",
    "\n",
    "LOG_FILENAME = data_directory + 'vec2topic.log'\n",
    "#logging.basicConfig(filename=LOG_FILENAME,level=logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s %(message)s',\"%b-%d-%Y %H:%M:%S\")\n",
    "logger.handlers[0].setFormatter(formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of intermediate files.\n",
    "---------------------------\n",
    "The (global) knowledge base is built off a (large) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    knowledge_base = 'viwiki-20160920-pages-articles.xml.bz2'\n",
    "else:\n",
    "    knowledge_base = 'vie_newscrawl_2011_1M.tkn.wseg'\n",
    "    \n",
    "knowledge_base_vector_dimension = 300    # Word vector dimensionality for knowledge base.\n",
    "knowledge_base_prefix = 'vie_newscrawl_2011_1M'\n",
    "\n",
    "\n",
    "knowledge_base_text = data_directory + knowledge_base\n",
    "knowledge_base_phrases = data_directory + knowledge_base_prefix + '_phrases.txt'\n",
    "\n",
    "knowledge_base_model = model_directory + knowledge_base_prefix + '.bin'\n",
    "knowledge_base_vectors = model_directory + knowledge_base_prefix + '.vec'\n",
    "knowledge_base_vectors_tsne = model_directory + knowledge_base_prefix + '_vec_tsne.txt'\n",
    "knowledge_base_vocab = model_directory + knowledge_base_prefix + '_vocab.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (local) user generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_content_name = 'OpenSubtitles2016_xml_vi_2015_369610_6346303'\n",
    "local_content_vector_dimension = 25\n",
    "\n",
    "local_content = data_directory + local_content_name\n",
    "\n",
    "## Intermediate files associated in proccessing input with external Java package JVnTextPro.\n",
    "local_content_xml = local_content + '.xml'\n",
    "local_content_txt = local_content + '.txt'\n",
    "local_content_txt_sent = local_content + '.txt.sent'\n",
    "local_content_txt_sent_tkn = local_content + '.txt.sent.tkn'\n",
    "local_content_txt_sent_tkn_wseg = local_content + '.txt.sent.tkn.wseg'\n",
    "local_content_txt_sent_tkn_wseg_pos = local_content + '.txt.sent.tkn.wseg.pos'\n",
    "\n",
    "## Intermediate files resulting from computation of word embeddings using fastText package.\n",
    "local_content_vectors = model_directory + local_content_name + '.vec'\n",
    "local_content_model = model_directory + local_content_name + '.bin'\n",
    "\n",
    "## Projected 2D vectors useful for visualization.\n",
    "local_content_vectors_tsne = model_directory + local_content_name + '_vec_tsne.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_vectors = model_directory + local_content_name + '.combined_vectors.txt'\n",
    "combined_vectors_tsne = model_directory + local_content_name + '.combined_vectors_tsne.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global knowledge vectors -- wikipedia & Leipzig Corpora\n",
    "-------------------------------------------------------\n",
    "First step is to compute word embeddings of a global knowledge base (e.g. wikipedia or the Leipzig Corpora) to capture the generic meaning of words in widely used contexts.\n",
    "\n",
    "The gensim package has examples of processing wikipedia dumps as well as streaming corpus implementation. The article just glosses over these steps and the sample github code grabs an undocumented data set from the authors drobbox account. In the cells below we rely on word2vec:\n",
    "<pre>\n",
    "git clone https://github.com/tmikolov/word2vec.git\n",
    "</pre>\n",
    "Also, in order to compute the t-sne embeddings with a c-language program, used bhtsne:\n",
    "<pre>\n",
    "git clone https://github.com/lvdmaaten/bhtsne.git\n",
    "</pre>\n",
    "When using jupyter-gallery docker image, usually install these in the /root directory. Hardwired into this notebook. \n",
    "\n",
    "**TODO:** \n",
    "* Parse the wikipedia dump name and use it as the prefix for the other intermediate files.\n",
    "* Download a wikipedia dump if it doesn't already exist.\n",
    "* Make things work for other languages (hundreds of wikipedias).\n",
    "* Check that WikiCorpus does lowercase each word.\n",
    "* Handle stopwords and substitution lists consistently.\n",
    "* Stem global and local data sets.\n",
    "* Check to see any value of using textblob over nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process wikipedia dump\n",
    "First download the wikipedia dump and place it in the data directory before running this notebook. The cell below will use the gensim class WikiCorpus to strip the wikipedia markup and store each article as one line of the output text file. Only do these computations once if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(knowledge_base_text):\n",
    "    space = ' '\n",
    "    i = 0\n",
    "    output = open(knowledge_base_text, 'w')\n",
    "    logger.info('Processing knowledge base %s', knowledge_base)\n",
    "    wiki = WikiCorpus(data_directory + knowledge_base, lemmatize=False, dictionary={})\n",
    "    for text in wiki.get_texts():\n",
    "        output.write(space.join(text) + \"\\n\")\n",
    "        i = i + 1\n",
    "        if (i % 10000 == 0):\n",
    "            logger.info(\"Saved \" + str(i) + \" articles\")\n",
    "    output.close()\n",
    "    logger.info(\"Finished Saved \" + str(i) + \" articles\")\n",
    "else:\n",
    "    logger.info('Knowledge base %s already on disk.', knowledge_base_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute word vectors for knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(knowledge_base_vectors):\n",
    "    knowledge_base_skipgram = fasttext.skipgram(knowledge_base_text, knowledge_base_vectors, lr=0.02, \n",
    "        dim=knowledge_base_vector_dimension, ws=5,\n",
    "        epoch=1, min_count=5, neg=5, loss='ns', bucket=2000000, minn=3, maxn=6,\n",
    "        thread=8, t=1e-4, lr_update_rate=100)\n",
    "else:\n",
    "    logger.info('Knowledge vectors %s already on disk.', knowledge_base_vectors)\n",
    "    knowledge_base_skipgram = fasttext.load_model(knowledge_base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple test to see if the model created/read ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print u'siêu_thị' in knowledge_base_skipgram\n",
    "print u'supermarket' in knowledge_base_skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a counter to keep track of the knowledge base vocabulary. Later the sample code uses this to find the vocabulary in common between the knowledge base and the user generated data. Try to process both data sets in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knowledge_base_exist = Counter()\n",
    "for w in knowledge_base_skipgram.words:\n",
    "    knowledge_base_exist[w.lower()] = w.lower()\n",
    "knowledge_base_vocab_lowercase = knowledge_base_exist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info(u'siêu_thị: %s', knowledge_base_exist[u'siêu_thị'])\n",
    "logger.info('funky: %s', knowledge_base_exist[u'funky'])\n",
    "logger.info('san_diego: %s', knowledge_base_exist[u'san_diego'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User content vectors -- OpenSubtitles2016\n",
    "-----------------------------------------\n",
    "OpenSubtitles is a very useful project for language analysis since it has a decent collection of parrallel sentences -- the foreign language captions that enthusiasts have created for their favorite movies.\n",
    "\n",
    "---\n",
    "Start with an `input.xml`, file listing captions from foreign film' obtained from the OpenSubtitle project. The final segmented text is input_text.txt.wseg (local_content_txt_sent_tkn_wseg). The parts of speech tagging is required in order to strip out the nouns (better labels for topics).\n",
    "\n",
    "<pre>\n",
    "BeautifulSoup:                     input.xml -> input.txt \n",
    "                           local_content_xml -> local_content_txt   \n",
    "JVnSenSegmenter:                   input.txt -> input.txt.sent\n",
    "                           local_content_txt -> local_content_txt_sent   \n",
    "JVnTokenizer:                 input.txt.sent -> input.txt.sent.tkn\n",
    "                      local_content_txt_sent -> local_content_txt_sent_tkn         \n",
    "JVnSegmenter:             input.txt.sent.tkn -> input.txt.sent.tkn.wseg\n",
    "                  local_content_txt_sent_tkn -> local_content_txt_sent_tkn_wseg  \n",
    "POSTagging:          input.txt.sent.tkn.wseg -> input.txt.sent.tkn.wseg.pos\n",
    "             local_content_txt_sent_tkn_wseg -> local_content_txt_sent_tkn_wseg_pos\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract text from data**\n",
    "\n",
    "Uses BeautifulSoup library to find all tags `'r'` and strip the text from them.\n",
    "\n",
    "TODO:\n",
    "1. Stream text through memory so that larger files can be proccessed.\n",
    "2. Allow for a directory of subfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(local_content_xml,'r') as fp:\n",
    "    soup = BeautifulSoup(fp,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(local_content_txt,'w') as fp:\n",
    "    for s in soup.findAll('s'): \n",
    "        fp.write(s.text.strip().encode('utf-8') + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cp = '/root/JVnTextPro/target/jvn-text-pro-2.0.jar:/root/.m2/repository/args4j/args4j/2.33/args4j-2.33.jar'\n",
    "model_dir = '/root/JVnTextPro/models/jvnsensegmenter/'\n",
    "java_class = 'jvnsensegmenter.JVnSenSegmenter'\n",
    "cmd = 'java -cp {} {} -modeldir {} -inputfile {}'.format(cp, java_class, model_dir, local_content_txt)\n",
    "output = os.popen(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Tokenization**\n",
    "\n",
    "Note: JVnTokenizer basically separates punctuation from words. Does not bother, for example, numbers like 22,216."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cp = '/root/JVnTextPro/target/jvn-text-pro-2.0.jar:/root/.m2/repository/args4j/args4j/2.33/args4j-2.33.jar'\n",
    "java_class = 'jvntokenizer.JVnTokenizer'\n",
    "cmd = 'java -cp {} {} -inputfile {}'.format(cp, java_class, local_content_txt_sent)\n",
    "output = os.popen(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cp = '/root/JVnTextPro/target/jvn-text-pro-2.0.jar:/root/.m2/repository/args4j/args4j/2.33/args4j-2.33.jar'\n",
    "model_dir = '/root/JVnTextPro/models/jvnsegmenter/'\n",
    "java_class = 'jvnsegmenter.WordSegmenting'\n",
    "cmd = 'java -cp {} {} -modeldir {}  -inputfile {}'.format(cp, java_class, model_dir, local_content_txt_sent_tkn)\n",
    "output = os.popen(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Seems to be a time delay before local_content_txt_sent_tkn_wseg is written to disk. Wait for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part of Speech Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cp = '/root/JVnTextPro/target/jvn-text-pro-2.0.jar:/root/.m2/repository/args4j/args4j/2.33/args4j-2.33.jar'\n",
    "model_dir = '/root/JVnTextPro/models/jvnpostag/maxent/'\n",
    "java_class = 'jvnpostag.POSTagging'\n",
    "cmd = 'java -cp {} {} -tagger maxent -modeldir {}  -inputfile {}'.format(cp, java_class, model_dir, \n",
    "                                                                         local_content_txt_sent_tkn_wseg)\n",
    "output = os.popen(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of stop words**\n",
    "\n",
    "The list below came from `elasticsearch` Vietnamese plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = [\"bị\", \"bởi\", \"cả\", \"các\", \"cái\", \"cần\", \"càng\", \"chỉ\", \"chiếc\", \"cho\", \"chứ\", \"chưa\", \"chuyện\",\n",
    "             \"có\", \"có_thể\", \"cứ\", \"của\", \"cùng\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đến nỗi\", \"đều\", \"điều\",\n",
    "             \"do\", \"đó\", \"được\", \"dưới\", \"gì\", \"khi\", \"không\", \"là\", \"lại\", \"lên\", \"lúc\", \"mà\", \"mỗi\", \"một_cách\",\n",
    "             \"này\", \"nên\", \"nếu\", \"ngay\", \"nhiều\", \"như\", \"nhưng\", \"những\", \"nơi\", \"nữa\", \"phải\", \"qua\", \"ra\",\n",
    "             \"rằng\", \"rằng\", \"rất\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"so\", \"sự\", \"tại\", \"theo\", \"thì\", \"trên\", \"trước\",\n",
    "             \"từ\", \"từng\", \"và\", \"vẫn\", \"vào\", \"vậy\", \"vì\", \"việc\", \"với\", \"vừa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Segmented Senteces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## After processing, each line is a sentence. \n",
    "## Read in lines, skipping empty lines, to yield\n",
    "text_lines = []\n",
    "with open(local_content_txt_sent_tkn_wseg,'rb') as local_content_file:\n",
    "    for line in local_content_file:\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            text_lines.append(line)\n",
    "for line in text_lines[0:3]:\n",
    "    logger.info('%s', line)\n",
    "logger.info(\"Text lines: %d\", len(text_lines))\n",
    "\n",
    "## The \n",
    "sentences = [[w for w in line.split()] for line in text_lines]\n",
    "for sent in sentences[0:3]:\n",
    "    logger.info('%s', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vietnamese Parts of Speech**\n",
    "1. N: Noun (danh từ)\n",
    "2. Np: Personal Noun (danh từ riêng)\n",
    "3. Nc: Classification Noun (danh từ chỉ loại)\n",
    "4. Nu: Unit Noun (danh từ đơn vị)\n",
    "5. V: verb (động từ)\n",
    "6. A: Adjective (tính từ)\n",
    "7. P: Pronoun (đại từ)\n",
    "8. L: attribute (định từ)\n",
    "9. M: Numeral (số từ)\n",
    "10. R: Adjunct (phụ từ) \n",
    "11. E: Preposition (giới từ)\n",
    "12. C: conjunction (liên từ)\n",
    "13. I: Interjection (thán từ)\n",
    "14. T: Particle, modal particle (trợ từ, tiểu từ)\n",
    "15. B: Words from foreign countries (Từ mượn tiếng nước ngoài ví dụ Internet, ...)\n",
    "16. Y: abbreviation (từ viết tắt)\n",
    "17. X: un-known (các từ không phân loại được)\n",
    "18. Mrk: punctuations (các dấu câu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_nouns_from_pos_data(path):\n",
    "    with open(path, 'rU') as data:\n",
    "        reader = csv.reader(data, delimiter=' ')\n",
    "        for row in reader:\n",
    "            nouns = []\n",
    "            for field in row:\n",
    "                try:\n",
    "                    word,pos = field.split('/')\n",
    "                    if pos in ['N','Np']: #,'Nc','Nu']:\n",
    "                        nouns.append(word)\n",
    "                except Exception as ex:\n",
    "                    logger.error('%s %s', ex, field)\n",
    "            yield nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_nouns = []\n",
    "count = 0\n",
    "for nouns in read_nouns_from_pos_data(local_content_txt_sent_tkn_wseg_pos): \n",
    "    if False and count > 10: \n",
    "        break\n",
    "    count += 1\n",
    "    sentences_nouns.append(nouns)\n",
    "    \n",
    "logger.info('%d', len(sentences_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute word vectors\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(local_content_vectors):\n",
    "    local_content_skipgram = fasttext.skipgram(local_content_txt_sent_tkn_wseg, model_directory + local_content_name, \n",
    "        lr=0.02, dim=local_content_vector_dimension, ws=5,\n",
    "        epoch=1, min_count=0, neg=5, loss='ns', bucket=2000000, minn=3, maxn=6,\n",
    "        thread=8, t=1e-5, lr_update_rate=100)\n",
    "else:\n",
    "    logger.info('Local vectors %s already on disk.', local_content_vectors)\n",
    "    local_content_skipgram = fasttext.load_model(local_content_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Creating word vecs')\n",
    "\n",
    "words=[w for text in sentences_nouns for w in text]\n",
    "Vocab=set(words)\n",
    "\n",
    "model_comb={}\n",
    "model_comb_vocab=[]\n",
    "\n",
    "common_vocab=set(knowledge_base_vocab_lowercase).intersection(local_content_skipgram.words).intersection(Vocab)\n",
    "\n",
    "for w in common_vocab:\n",
    "    if len(w)>2:\n",
    "        model_comb[w]=np.array(np.concatenate((knowledge_base_skipgram[w],local_content_skipgram[w])))\n",
    "        model_comb_vocab.append(w)\n",
    "        \n",
    "logger.info('Length of common_vocab = %d', len(common_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(set(knowledge_base_skipgram.words))\n",
    "print len(set(local_content_skipgram.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = csv.writer(open(combined_vectors,'w'),delimiter='\\t')\n",
    "for k in model_comb.keys():\n",
    "    writer.writerow(model_comb[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!/root/bhtsne/bhtsne.py -i {combined_vectors} -o {combined_vectors_tsne} -d 2 -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bokeh.charts import Scatter, output_notebook, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader = csv.reader(open(combined_vectors_tsne), delimiter='\\t')\n",
    "count = 0\n",
    "X_full = []\n",
    "for row in reader:\n",
    "    if False and count >= 10000:\n",
    "        break\n",
    "    X_full.append(np.array([float(p) for p in row]))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_full, columns=['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = Scatter(df, x='x', y='y', color='blue')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Create a frequency count of words in email\n",
    "words=[w for text in sentences_nouns for w in text]\n",
    "Vocab=set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Helper Functions\n",
    "def norm(a):\n",
    "    return np.sqrt(np.sum(np.square(a)))\n",
    "\n",
    "def cosine(a,b):\n",
    "    return 1-np.dot(a,b)/np.sqrt(np.sum(a**2)*np.sum(b**2))\n",
    "\n",
    "def l1(a,b):\n",
    "    return abs(a-b).sum()\n",
    "\n",
    "def l2(a,b):\n",
    "    return np.sqrt(np.square(a-b).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create a list of words to be clustered based on a model with some l2_threshold and can normalize the vectors \n",
    "### and also repeat or no\n",
    "def create_word_list(model,vocab,features,Texts,repeat=True,l2_threshold=0,normalized=True,min_count=100,min_length=0):\n",
    "    data_d2v=[]\n",
    "    word_d2v=[]\n",
    "    words_text=[w for text in Texts for w in text]\n",
    "    count=Counter(words_text)\n",
    "    if repeat:\n",
    "        for text in Texts:\n",
    "            for w in text:\n",
    "                if w in vocab and count[w]>min_count:\n",
    "                    if len(w)>min_length and l2(model[w],np.zeros(features))>l2_threshold:\n",
    "                        if normalized:\n",
    "                            data_d2v.append(model[w]/l2(model[w],np.zeros(features)))\n",
    "                        else:\n",
    "                            data_d2v.append(model[w])\n",
    "                        word_d2v.append(w)\n",
    "    else:\n",
    "        A=set(words_text)\n",
    "        for w in vocab:\n",
    "            if w in A and len(w)>min_length and l2(model[w],np.zeros(features))>l2_threshold and count[w]>min_count:\n",
    "                if normalized:\n",
    "                    data_d2v.append(model[w]/l2(model[w],np.zeros(features)))\n",
    "                else:\n",
    "                    data_d2v.append(model[w])\n",
    "                word_d2v.append(w)\n",
    "\n",
    "    return data_d2v, word_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run Agglomerative clustering\n",
    "logger.info('Clustering for depth...')\n",
    "local_vec = True\n",
    "\n",
    "data_d2v,word_d2v=create_word_list(model_comb,model_comb_vocab,25*local_vec+300,sentences_nouns,repeat=False,normalized=True,min_count=0,l2_threshold=0)\n",
    "spcluster=fastcluster.linkage(data_d2v,method='average',metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_depth(spcluster,words, num_points):\n",
    "    cluster=[[] for w in xrange(2*num_points)]\n",
    "    c=Counter()\n",
    "    for i in xrange(num_points):\n",
    "        cluster[i]=[i]\n",
    "\n",
    "    for i in xrange(len(spcluster)):\n",
    "        x=int(spcluster[i,0])\n",
    "        y=int(spcluster[i,1])\n",
    "        xval=[w for w in cluster[x]]\n",
    "        yval=[w for w in cluster[y]]\n",
    "        cluster[num_points+i]=xval+yval\n",
    "        for w in cluster[num_points+i]:\n",
    "            c[words[w]]+=1\n",
    "        cluster[x][:]=[]\n",
    "        cluster[y][:]=[]    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Calculate depth of words\n",
    "num_points=len(data_d2v)\n",
    "depth=calculate_depth(spcluster,word_d2v,num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    spcluster,\n",
    "    truncate_mode='lastp',\n",
    "    p=20,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=12.,  # font size for the x axis labels\n",
    "    show_contracted=True,\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Computing co-occurence graph')\n",
    "\n",
    "T=[' '.join(w) for w in sentences_nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in T[0:10]: print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info(len(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Co-occurence matrix\n",
    "cv=CountVectorizer(token_pattern=u'(?u)\\\\b([^\\\\s]+)')\n",
    "bow_matrix = cv.fit_transform(T)\n",
    "id2word={}\n",
    "for key, value in cv.vocabulary_.items():\n",
    "    id2word[value]=key\n",
    "\n",
    "ids=[]\n",
    "for key,value in cv.vocabulary_.iteritems():\n",
    "    if key in model_comb_vocab:\n",
    "        ids.append(value)\n",
    "\n",
    "sort_ids=sorted(ids)\n",
    "bow_reduced=bow_matrix[:,sort_ids]\n",
    "normalized = TfidfTransformer().fit_transform(bow_reduced)\n",
    "similarity_graph_reduced=bow_reduced.T * bow_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Depth-rank weighting of edges, weight of edge i,j=cosine of angle between them\n",
    "logger.info('Computing degree')\n",
    "m,n=similarity_graph_reduced.shape\n",
    "\n",
    "cx=similarity_graph_reduced.tocoo()\n",
    "keyz=[id2word[sort_ids[w]] for w in xrange(len(sort_ids))]\n",
    "data=[]\n",
    "ro=[]\n",
    "co=[]\n",
    "for i,j,v in itertools.izip(cx.row, cx.col, cx.data):\n",
    "    if v>0 and i!=j:\n",
    "        value=1\n",
    "        if value>0:\n",
    "            ro.append(i)\n",
    "            co.append(j)\n",
    "            data.append(value)\n",
    "\n",
    "SS=sp.sparse.coo_matrix((data, (ro, co)), shape=(m,n))\n",
    "SP_full=SS.tocsc()\n",
    "id_word={w:id2word[sort_ids[w]] for w in xrange(len(sort_ids))}\n",
    "word_id={value:key for key,value in id_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Computing metrics')\n",
    "#compute metrics\n",
    "degsum=SP_full.sum(axis=1)\n",
    "deg={}\n",
    "for x in xrange(len(sort_ids)):\n",
    "    deg[id2word[sort_ids[x]]]=int(degsum[x])\n",
    "\n",
    "max_deg=max(deg.values())\n",
    "max_depth=max(depth.values())\n",
    "\n",
    "temp_deg_mod={w:np.log(1+deg[w])/np.log(1+max_deg) for w in deg.iterkeys()}\n",
    "alpha=np.log(0.5)/np.log(np.median(temp_deg_mod.values()))\n",
    "deg_mod={key:value**alpha for key, value in temp_deg_mod.iteritems()}\n",
    "\n",
    "temp={key:value*1./max_depth for key, value in depth.iteritems()}\n",
    "alpha=np.log(0.5)/np.log(np.median(temp.values()))\n",
    "depth_mod={key:value**alpha for key, value in temp.iteritems()}\n",
    "\n",
    "temp={key:deg_mod[key]*depth_mod[key] for key in depth_mod.iterkeys()}\n",
    "max_metric=np.max(temp.values())\n",
    "metric={key:value*1./max_metric for key,value in temp.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('max_deg = %s, max_depth = %s',max_deg, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Kmeans\n",
    "NUM_TOPICS = 150\n",
    "K=NUM_TOPICS\n",
    "kmeans=KMeans(n_clusters=K)\n",
    "kmeans.fit([w for w in data_d2v])\n",
    "kmeans_label={word_d2v[x]:kmeans.labels_[x] for x in xrange(len(word_d2v))}\n",
    "\n",
    "kmeans_label_ranked={}\n",
    "\n",
    "topic=[[] for i in xrange(K)]\n",
    "clust_depth=[[] for i in xrange(K)]\n",
    "for i in xrange(K):\n",
    "    topic[i]=[word_d2v[x] for x in xrange(len(word_d2v)) if kmeans.labels_[x]==i]\n",
    "    temp_score=[metric[w] for w in topic[i]]\n",
    "    clust_depth[i]=-np.mean(sorted(temp_score,reverse=True)[:])#int(np.sqrt(len(topic[i])))])\n",
    "index=np.argsort(clust_depth)\n",
    "for num,i in enumerate(xrange(K)):\n",
    "    for w in topic[index[i]]:\n",
    "        kmeans_label_ranked[w]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Done...Generating output')\n",
    "lister=[]\n",
    "to_show=K\n",
    "to_show_words=20 #the maximum number of words of each type to display\n",
    "for i in xrange(to_show):\n",
    "    top=topic[index[i]]\n",
    "    sort_top=[w[0] for w in sorted([[w,metric[w]] for w in top],key=itemgetter(1),reverse=True)]\n",
    "    lister.append(['Topic %d' %(i+1)]+sort_top[:to_show_words])\n",
    "\n",
    "max_len=max([len(w) for w in lister])\n",
    "new_list=[]\n",
    "for list_el in lister:\n",
    "    new_list.append(list_el + [''] * (max_len - len(list_el)))\n",
    "Topics=list(itertools.izip_longest(*new_list))\n",
    "#X.insert(len(X),[-int(clust_depth[index[w]]*100)*1./100 for w in xrange(K)])\n",
    "sorted_words=[w[0] for w in sorted(metric.items(),key=itemgetter(1),reverse=True)][:to_show_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_words = sorted_words\n",
    "deep_words = [w[0] for w in depth.most_common(to_show_words)]\n",
    "filer = 'wiki_simple.txt'\n",
    "outfile_topics = data_directory + filer.split('.')[0] + '_topics.csv'\n",
    "outfile_score = data_directory + filer.split('.')[0] + '_score.csv'\n",
    "outfile_depth = data_directory + filer.split('.')[0] + '_depth.csv'\n",
    "b = open(outfile_topics, 'wb')\n",
    "a = csv.writer(b)\n",
    "a.writerows(Topics)\n",
    "b = open(outfile_score, 'wb')\n",
    "a = csv.writer(b)\n",
    "a.writerows([[w] for w in score_words])\n",
    "b = open(outfile_depth, 'wb')\n",
    "a = csv.writer(b)\n",
    "a.writerows([[w] for w in deep_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step = 10\n",
    "for j in range(K/step + 1):\n",
    "    first = j*step + 1;last = j*step + step\n",
    "    print 'Total number of Topics = {}. Displaying Topics {} thru {}.'.format(K, first, last)\n",
    "    print tabulate([Topics[i][first-1:last] for i in range(0,21)], tablefmt=u'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
