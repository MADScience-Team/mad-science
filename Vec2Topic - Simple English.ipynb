{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling Using Distributed Word Embeddings\n",
    "================================================\n",
    "Notebook version of https://github.com/rsrandhawa/Vec2Topic code, based on the article \"Topic Modeling Using Distributed Word Embeddings\" by R. S. Randhawa, P. Jain, and G. Madan. \n",
    "\n",
    "The basic approach is to first create a language model based on a large (ideally billions of words) text corpus. The technology used, distributed word embeddings, is a shallow neural network that seems to perform best on large datasets (trades simple but fast computation for tons of data).\n",
    "\n",
    "The user generated content (which is usually a much smaller corpus) is likewise trained with consistent parameters. Vectors corresponding to the same vocabulary word are concatenated together to provide a model of the user generated content.\n",
    "\n",
    "Word vectors that cluster together are interperted as topics of the user generated content. Some clusters appear better than others because they consist of coherent lists of words -- main goal is to score the importance of each topic.\n",
    "\n",
    "Performing a hierarchical clustering provides a measure of depth for each word and computing a co-occurance graph (edge between two words if they belong to the same sentenence) provides a degree of co-occurance. Each word is scored by a (normalized) product of depth and degree. KMeans is used to cluster words into topics, and the scoring function is used to order the words and the topics.\n",
    "\n",
    "Notebook below uses `fasttext` instead of `word2vec`. Looks like `fasttext` is showing too much prefernce to word endings (used default of 3-6 ngrams).\n",
    "\n",
    "<pre>\n",
    "+---------------+---------------+-----------+------------+-----------+-------------+--------------+-------------+\n",
    "| Topic 1       | Topic 2       | Topic 3   | Topic 4    | Topic 5   | Topic 6     | Topic 7      | Topic 8     |\n",
    "| investment    | costs         | meeting   | future     | customers | security    | opportunity  | market      |\n",
    "| management    | opportunities | marketing | enterprise | markets   | asset       | poverty      | trade       |\n",
    "| response      | operations    | planning  | interest   | offers    | agreement   | debt         | summit      |\n",
    "| globalization | activities    | waiting   | business   | savings   | alliance    | tax          | fax         |\n",
    "| process       | securities    | trading   | compaq     | partners  | seminar     | peacekeeping | pipeline    |\n",
    "| conversation  | assets        | pricing   | demand     | remarks   | mission     | expense      | marketplace |\n",
    "| stabilization | strategies    | housing   | customer   | others    | leadership  | ideal        | lay         |\n",
    "| ability       | solutions     | opening   | exchange   | meetings  | office      | audience     | shoreline   |\n",
    "| invitation    | investments   | evening   | power      | seminars  | service     | awareness    | street      |\n",
    "| integration   | analysts      | sizing    | balance    | stories   | partnership | threat       | deep-water  |\n",
    "+---------------+---------------+-----------+------------+-----------+-------------+--------------+-------------+\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required standard packages\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging, re, os, bz2, pickle\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Unicode wrapper for reading & writing csv files.\n",
    "import unicodecsv as csv\n",
    "\n",
    "## Lighter weight than pandas -- tabular display of tables.\n",
    "from tabulate import tabulate\n",
    "\n",
    "## In order to strip out the text from the xml formated data.\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required data science packages\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## First the usual suspects: numpy, scipy, and gensim\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gensim\n",
    "\n",
    "## For scraping text out of a wikipedia dump. Get dumps at https://dumps.wikimedia.org/backup-index.html\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "## For computing phrases from input text.\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "## Latest greatest word vectors (see https://pypi.python.org/pypi/fasttext).\n",
    "import fasttext\n",
    "\n",
    "## Latest greatest hierarchical clustering package. \n",
    "## Word vectors are clustered, with deeper trees indicating core topics.\n",
    "import fastcluster\n",
    "\n",
    "## Use scikit-learn to generate co-occurancy graph (edge if words in same sentence).\n",
    "## The degree of each word indicates how strong it co-occurs.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "## Use scikit-learn for K-Means clustering: identify topics.\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base data directory and logging.\n",
    "--------------------------------\n",
    "The approach currently uses a lot of intermediate files (which is annoying, but means that the project can work on machines with smaller physical memory). The initial data (knowledge base as well as user generated content) and the intermediate files are all kept in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_directory = 'data/'\n",
    "model_directory = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "reload(logging)\n",
    "\n",
    "LOG_FILENAME = data_directory + 'vec2topic.log'\n",
    "#logging.basicConfig(filename=LOG_FILENAME,level=logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s %(message)s',\"%b-%d-%Y %H:%M:%S\")\n",
    "logger.handlers[0].setFormatter(formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of intermediate files.\n",
    "---------------------------\n",
    "The (global) knowledge base is built off a (large) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Main inputs to program. Data for model and name of knowledge base (background language model).\n",
    "knowledge_base = 'simplewiki-20160820-pages-articles.xml.bz2'\n",
    "knowledge_base_prefix = 'simplewiki-20160820-pages-articles'\n",
    "knowledge_base_vector_dimension = 200    # Word vector dimension for knowledge base.\n",
    "\n",
    "## Intermediate files generated from inputs.\n",
    "knowledge_base_text = data_directory + knowledge_base_prefix + '.txt'\n",
    "knowledge_base_phrases = data_directory + knowledge_base_prefix + '_phrases.txt'\n",
    "knowledge_base_model = model_directory + knowledge_base_prefix + '.bin'\n",
    "knowledge_base_vectors = model_directory + knowledge_base_prefix + '.vec'\n",
    "knowledge_base_vectors_tsne = model_directory + knowledge_base_prefix + '_vec_tsne.txt'\n",
    "knowledge_base_vocab = model_directory + knowledge_base_prefix + '_vocab.txt'\n",
    "knowledge_base_bigrams = data_directory + knowledge_base_prefix + '_bigrams.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (local) user generated content. Sample data from OpenSubtitles: http://opus.lingfil.uu.se/OpenSubtitles2016/xml/en/2015/369610/6300079.xml.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Main inputs to program (data for user content and name of local model).\n",
    "local_content_name = 'ken_lay_text'\n",
    "#local_content_name = 'OpenSubtitles2016_xml_en_2015_369610_6300079'\n",
    "local_content_vector_dimension = 25\n",
    "\n",
    "## File names for user content.\n",
    "local_content = data_directory + local_content_name \n",
    "local_content_xml = local_content + '.xml'\n",
    "local_content_txt = local_content + '.txt'\n",
    "local_content_phrases = local_content + '_phrases.txt'\n",
    "\n",
    "## Intermediate files resulting from computation of word embeddings using fastText package.\n",
    "local_content_vectors = model_directory + local_content_name + '.vec'\n",
    "local_content_model = model_directory + local_content_name + '.bin'\n",
    "\n",
    "## Projected 2D vectors useful for visualization.\n",
    "#local_content_vectors_tsne = data_directory + local_content_prefix + '_vec_tsne.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_vectors = model_directory + local_content_name + '.combined_vectors.txt'\n",
    "combined_vectors_tsne = model_directory + local_content_name + '.combined_vectors_tsne.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global knowledge vectors -- English Wikipedi\n",
    "--------------------------------------------\n",
    "First step is to compute word embeddings of a global knowledge base from the English Wikipedia to capture the generic meaning of words in widely used contexts.\n",
    "\n",
    "The gensim package has examples of processing wikipedia dumps as well as streaming corpus implementation. The article just glosses over these steps and the sample github code grabs an undocumented data set from the authors drobbox account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process wikipedia dump\n",
    "First download the wikipedia dump and place it in the data directory before running this notebook. The cell below will use the gensim class WikiCorpus to strip the wikipedia markup and store each article as one line of the output text file. Only do these computations once if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knowledge_base_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(knowledge_base_text):\n",
    "    space = ' '\n",
    "    i = 0\n",
    "    output = open(knowledge_base_text, 'wb')\n",
    "    logger.info('Processing knowledge base %s', knowledge_base)\n",
    "    wiki = WikiCorpus(data_directory + knowledge_base, lemmatize=False, dictionary={})\n",
    "    for text in wiki.get_texts():\n",
    "        output.write(space.join(text) + \"\\n\")\n",
    "        i = i + 1\n",
    "        if (i % 10000 == 0):\n",
    "            logger.info(\"Saved \" + str(i) + \" articles\")\n",
    "    output.close()\n",
    "    logger.info(\"Finished Saved \" + str(i) + \" articles\")\n",
    "else:\n",
    "    logger.info('Knowledge base %s already on disk.', knowledge_base_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Use gensim to compute phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls {data_directory + knowledge_base}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "fp = bz2.BZ2File(data_directory + knowledge_base,'rU')\n",
    "for title, text, pageid in gensim.corpora.wikicorpus.extract_pages(fp):\n",
    "#for (tokens, (pageid, title)) in wiki.get_texts():\n",
    "    if count >= 1: break\n",
    "    count += 1\n",
    "    text = gensim.corpora.wikicorpus.filter_wiki(text)\n",
    "    sents = nltk.sent_tokenize(text.lower())\n",
    "    print pageid, title\n",
    "    for sent in sents:\n",
    "        print sent\n",
    "        \n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_sents_from_data(path):\n",
    "    with bz2.BZ2File(path, 'rb') as data:    \n",
    "        for title, text, pageid in gensim.corpora.wikicorpus.extract_pages(data):\n",
    "            text = gensim.corpora.wikicorpus.filter_wiki(text)\n",
    "            sents = nltk.sent_tokenize(text.lower())\n",
    "            for sent in sents:\n",
    "                yield nltk.word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sent in read_sents_from_data(data_directory + knowledge_base):\n",
    "    if True and count >= 10:\n",
    "        break\n",
    "    count += 1\n",
    "    print sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO:* Determine optimal values of `max_vocab_size` for 16G RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kb_bigrams = Phrases(read_sents_from_data(data_directory + knowledge_base), threshold=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for phrase, score in kb_bigrams.export_phrases(read_sents_from_data(data_directory + knowledge_base)):\n",
    "    print(u'{0}\\t{1}'.format(phrase, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(knowledge_base_bigrams):\n",
    "    with open(knowledge_base_bigrams,'w') as bigrams_fp:\n",
    "        pickle.dump(kb_bigrams, bigrams_fp)\n",
    "    logger.info('Saved copy of knowledge base bigrams %s', knowledge_base_bigrams)\n",
    "else:\n",
    "    with open(knowledge_base_bigrams,'r') as bigrams_fp:\n",
    "        pickle.load(kb_bigrams, bigrams_fp)    \n",
    "    logger.info('Read copy of knowledge base bigrams %s', knowledge_base_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for s in kb_trigrams[[sent.split() for sent in sents]]:\n",
    "#    print ' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#kb_trigrams = Phrases(kb_bigrams[read_sents_from_data(data_directory + knowledge_base)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for phrase, score in kb_trigrams.export_phrases(read_sents_from_data(data_directory + knowledge_base)):\n",
    "#    print(u'{0}\\t{1}'.format(phrase, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(knowledge_base_phrases):\n",
    "    with open(knowledge_base_phrases, 'w') as data:\n",
    "        for sent in read_sents_from_data(data_directory + knowledge_base):\n",
    "            s = ' '.join(kb_bigrams[sent]) + u'\\n'\n",
    "            data.write(s.encode('utf-8'))\n",
    "    logger.info('Saved copy of knowledge base phrases %s', knowledge_base_phrases)\n",
    "else:    \n",
    "    logger.info('Copy of knowledge base phrases %s on disk.', knowledge_base_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute word vectors for knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some computational performances comparing `word2vec` vs. `fasttext`. \n",
    "\n",
    "For computing full wikipedia using `word2vec`, using 300 dimensional word vectors, need to filter vocabulary so that basic memory usage of word2vec fits in physical memory. \n",
    "\n",
    "> the `syn0` structure holding (input) word-vectors-in-training will require:\n",
    "> 5759121 (your vocab size) * 600 (dimensions) * 4 bytes/dimension = 13.8GB\n",
    "> The `syn1neg` array (hidden->output weights) will require another 13.8GB.\n",
    "<pre>\n",
    "min_count = 10 results in 2,947,700 words (requires more than 7G physical memory)\n",
    "min_count = 5 results in 4,733,171 words (requires more than 11G physical memory)\n",
    "min_count = 0 results in 11,631,317 words (requires more than 28G physical memory)\n",
    "</pre>\n",
    "\n",
    "Using `fasttext` with `min_count=5`, `bucket=2000000`, and `t=1e-4` on enwiki, used a constant 8.43G memory used during computation (over 10 hours 8-core, 16G ram). Final vocabulary has 2,114,311 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(knowledge_base_vectors):\n",
    "    knowledge_base_skipgram = fasttext.skipgram(knowledge_base_phrases, \n",
    "        model_directory + knowledge_base_prefix, lr=0.02, \n",
    "        dim=knowledge_base_vector_dimension, ws=5, word_ngrams=1,\n",
    "        epoch=1, min_count=5, neg=5, loss='ns', bucket=2000000, minn=3, maxn=6,\n",
    "        thread=8, t=1e-4, lr_update_rate=100)\n",
    "else:\n",
    "    logger.info('Knowledge vectors %s already on disk.', knowledge_base_vectors)\n",
    "    knowledge_base_skipgram = fasttext.load_model(knowledge_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(knowledge_base_skipgram.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple test to see if the model created/read ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print u'supermarket' in knowledge_base_skipgram\n",
    "print u'san_diego' in knowledge_base_skipgram\n",
    "print u'San_Diego' in knowledge_base_skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a counter to keep track of the knowledge base vocabulary. Later the sample code uses this to find the vocabulary in common between the knowledge base and the user generated data. Try to process both data sets in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knowledge_base_exist = Counter()\n",
    "for w in knowledge_base_skipgram.words:\n",
    "    knowledge_base_exist[w.lower()] = w.lower()\n",
    "knowledge_base_vocab_lowercase = knowledge_base_exist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('funky: %s', knowledge_base_exist[u'funky'])\n",
    "logger.info('san_diego: %s', knowledge_base_exist[u'san_diego'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User content vectors -- OpenSubtitles2016\n",
    "-----------------------------------------\n",
    "OpenSubtitles is a very useful project for language analysis since it has a decent collection of parrallel sentences -- the foreign language captions that enthusiasts have created for their favorite movies.\n",
    "\n",
    "---\n",
    "Start with an `input.xml`, file listing captions from foreign film' obtained from the OpenSubtitle project. \n",
    "\n",
    "<pre>\n",
    "BeautifulSoup:                     input.xml -> input.txt \n",
    "                           local_content_xml -> local_content_txt   \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_content_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(local_content_xml,'r') as fp:\n",
    "    soup = BeautifulSoup(fp,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(local_content_txt,'w') as fp:\n",
    "    for s in soup.findAll('s'): \n",
    "        text = ' '.join(s.text.strip().lower().split())\n",
    "        fp.write(text.encode('utf-8') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## After processing, each line is a sentence. \n",
    "## Read in lines, skipping empty lines, to yield\n",
    "text_lines = []\n",
    "with open(local_content_txt, 'rb') as local_content_file:\n",
    "    for line in local_content_file:\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            text_lines.append(line)\n",
    "for line in text_lines[0:3]:\n",
    "    logger.info('%s', line)\n",
    "logger.info(\"Text lines: %d\", len(text_lines))\n",
    "\n",
    "## The \n",
    "sentences = [[w for w in line.split()] for line in text_lines]\n",
    "for sent in sentences[0:3]:\n",
    "    logger.info('%s', sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lc_bigrams = kb_bigrams[sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for phrase, score in kb_bigrams.export_phrases(sentences):\n",
    "    print(u'{0}\\t{1}'.format(phrase, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open(local_content_phrases, 'w') as data:\n",
    "    for sent in kb_bigrams[sentences]:\n",
    "        if False and count >= 100: break\n",
    "        count += 1\n",
    "        s = ' '.join(sent) + u'\\n'\n",
    "        data.write(s.lower().encode('utf-8'))\n",
    "        \n",
    "logger.info('Wrote %s sentences.', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_nouns_from_pos_data(path):\n",
    "    with open(path, 'rU') as data:\n",
    "        reader = csv.reader(data, delimiter=' ')\n",
    "        for row in reader:\n",
    "            nouns = []\n",
    "            blob=TextBlob(' '.join(row))\n",
    "            for word,tag in blob.tags:\n",
    "                if tag in ['NN','NNP','NNS','NNPS']:\n",
    "                    nouns.append(word) \n",
    "            yield nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences_nouns = []\n",
    "count = 0\n",
    "for nouns in read_nouns_from_pos_data(local_content_phrases): \n",
    "    if False and count > 10: \n",
    "        break\n",
    "    count += 1\n",
    "    sentences_nouns.append(nouns)\n",
    "    \n",
    "logger.info('%d', len(sentences_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute word vectors\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_directory + local_content_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(local_content_vectors):\n",
    "    local_content_skipgram = fasttext.skipgram(local_content_phrases, model_directory + local_content_name, \n",
    "        lr=0.02, dim=local_content_vector_dimension, ws=5, word_ngrams=1,\n",
    "        epoch=1, min_count=0, neg=5, loss='ns', bucket=2000000, minn=3, maxn=6,\n",
    "        thread=8, t=1e-4, lr_update_rate=100)\n",
    "else:\n",
    "    logger.info('Local vectors %s already on disk.', local_content_vectors)\n",
    "    local_content_skipgram = fasttext.load_model(local_content_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Creating word vecs')\n",
    "\n",
    "words=[w for text in sentences_nouns for w in text]\n",
    "Vocab=set(words)\n",
    "\n",
    "model_comb={}\n",
    "model_comb_vocab=[]\n",
    "\n",
    "common_vocab=set(knowledge_base_vocab_lowercase).intersection(local_content_skipgram.words).intersection(Vocab)\n",
    "\n",
    "for w in common_vocab:\n",
    "    if len(w)>2:\n",
    "        model_comb[w]=np.array(np.concatenate((knowledge_base_skipgram[w],local_content_skipgram[w])))\n",
    "        model_comb_vocab.append(w)\n",
    "    else:\n",
    "        logger.info(w)\n",
    "        \n",
    "logger.info('Length of common_vocab = %d', len(common_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(set(knowledge_base_skipgram.words))\n",
    "print len(set(local_content_skipgram.words))\n",
    "print len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = csv.writer(open(combined_vectors,'w'),delimiter='\\t')\n",
    "for k in model_comb.keys():\n",
    "    writer.writerow(model_comb[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!/root/bhtsne/bhtsne.py -i {combined_vectors} -o {combined_vectors_tsne} -d 2 -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bokeh.charts import Scatter, output_notebook, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader = csv.reader(open(combined_vectors_tsne), delimiter='\\t')\n",
    "count = 0\n",
    "X_full = []\n",
    "for row in reader:\n",
    "    if False and count >= 10000:\n",
    "        break\n",
    "    X_full.append(np.array([float(p) for p in row]))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_full, columns=['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = Scatter(df, x='x', y='y', color='blue')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Create a frequency count of words in email\n",
    "words=[w for text in sentences_nouns for w in text]\n",
    "Vocab=set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Helper Functions\n",
    "def norm(a):\n",
    "    return np.sqrt(np.sum(np.square(a)))\n",
    "\n",
    "def cosine(a,b):\n",
    "    return 1-np.dot(a,b)/np.sqrt(np.sum(a**2)*np.sum(b**2))\n",
    "\n",
    "def l1(a,b):\n",
    "    return abs(a-b).sum()\n",
    "\n",
    "def l2(a,b):\n",
    "    return np.sqrt(np.square(a-b).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create a list of words to be clustered based on a model with some l2_threshold and can normalize the vectors \n",
    "### and also repeat or no\n",
    "def create_word_list(model,vocab,features,Texts,repeat=True,l2_threshold=0,normalized=True,min_count=100,min_length=0):\n",
    "    data_d2v=[]\n",
    "    word_d2v=[]\n",
    "    words_text=[w for text in Texts for w in text]\n",
    "    count=Counter(words_text)\n",
    "    if repeat:\n",
    "        for text in Texts:\n",
    "            for w in text:\n",
    "                if w in vocab and count[w]>min_count:\n",
    "                    if len(w)>min_length and l2(model[w],np.zeros(features))>l2_threshold:\n",
    "                        if normalized:\n",
    "                            data_d2v.append(model[w]/l2(model[w],np.zeros(features)))\n",
    "                        else:\n",
    "                            data_d2v.append(model[w])\n",
    "                        word_d2v.append(w)\n",
    "    else:\n",
    "        A=set(words_text)\n",
    "        for w in vocab:\n",
    "            if w in A and len(w)>min_length and l2(model[w],np.zeros(features))>l2_threshold and count[w]>min_count:\n",
    "                if normalized:\n",
    "                    data_d2v.append(model[w]/l2(model[w],np.zeros(features)))\n",
    "                else:\n",
    "                    data_d2v.append(model[w])\n",
    "                word_d2v.append(w)\n",
    "\n",
    "    return data_d2v, word_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run Agglomerative clustering\n",
    "logger.info('Clustering for depth...')\n",
    "local_vec = True\n",
    "\n",
    "data_d2v,word_d2v=create_word_list(model_comb,model_comb_vocab,25*local_vec+200,sentences_nouns,repeat=False,normalized=True,min_count=0,l2_threshold=0)\n",
    "spcluster=fastcluster.linkage(data_d2v,method='average',metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_depth(spcluster,words, num_points):\n",
    "    cluster=[[] for w in xrange(2*num_points)]\n",
    "    c=Counter()\n",
    "    for i in xrange(num_points):\n",
    "        cluster[i]=[i]\n",
    "\n",
    "    for i in xrange(len(spcluster)):\n",
    "        x=int(spcluster[i,0])\n",
    "        y=int(spcluster[i,1])\n",
    "        xval=[w for w in cluster[x]]\n",
    "        yval=[w for w in cluster[y]]\n",
    "        cluster[num_points+i]=xval+yval\n",
    "        for w in cluster[num_points+i]:\n",
    "            c[words[w]]+=1\n",
    "        cluster[x][:]=[]\n",
    "        cluster[y][:]=[]    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Calculate depth of words\n",
    "num_points=len(data_d2v)\n",
    "depth=calculate_depth(spcluster,word_d2v,num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    spcluster,\n",
    "    truncate_mode='lastp',\n",
    "    p=20,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=12.,  # font size for the x axis labels\n",
    "    show_contracted=True,\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Computing co-occurence graph')\n",
    "\n",
    "T=[' '.join(w) for w in sentences_nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in T[0:10]: print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info(len(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Co-occurence matrix\n",
    "cv=CountVectorizer(token_pattern=u'(?u)\\\\b([^\\\\s]+)')\n",
    "bow_matrix = cv.fit_transform(T)\n",
    "id2word={}\n",
    "for key, value in cv.vocabulary_.items():\n",
    "    id2word[value]=key\n",
    "\n",
    "ids=[]\n",
    "for key,value in cv.vocabulary_.iteritems():\n",
    "    if key in model_comb_vocab:\n",
    "        ids.append(value)\n",
    "\n",
    "sort_ids=sorted(ids)\n",
    "bow_reduced=bow_matrix[:,sort_ids]\n",
    "normalized = TfidfTransformer().fit_transform(bow_reduced)\n",
    "similarity_graph_reduced=bow_reduced.T * bow_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Depth-rank weighting of edges, weight of edge i,j=cosine of angle between them\n",
    "logger.info('Computing degree')\n",
    "m,n=similarity_graph_reduced.shape\n",
    "\n",
    "cx=similarity_graph_reduced.tocoo()\n",
    "keyz=[id2word[sort_ids[w]] for w in xrange(len(sort_ids))]\n",
    "data=[]\n",
    "ro=[]\n",
    "co=[]\n",
    "for i,j,v in itertools.izip(cx.row, cx.col, cx.data):\n",
    "    if v>0 and i!=j:\n",
    "        value=1\n",
    "        if value>0:\n",
    "            ro.append(i)\n",
    "            co.append(j)\n",
    "            data.append(value)\n",
    "\n",
    "SS=sp.sparse.coo_matrix((data, (ro, co)), shape=(m,n))\n",
    "SP_full=SS.tocsc()\n",
    "id_word={w:id2word[sort_ids[w]] for w in xrange(len(sort_ids))}\n",
    "word_id={value:key for key,value in id_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Computing metrics')\n",
    "#compute metrics\n",
    "degsum=SP_full.sum(axis=1)\n",
    "deg={}\n",
    "for x in xrange(len(sort_ids)):\n",
    "    deg[id2word[sort_ids[x]]]=int(degsum[x])\n",
    "\n",
    "max_deg=max(deg.values())\n",
    "max_depth=max(depth.values())\n",
    "\n",
    "temp_deg_mod={w:np.log(1+deg[w])/np.log(1+max_deg) for w in deg.iterkeys()}\n",
    "alpha=np.log(0.5)/np.log(np.median(temp_deg_mod.values()))\n",
    "deg_mod={key:value**alpha for key, value in temp_deg_mod.iteritems()}\n",
    "\n",
    "temp={key:value*1./max_depth for key, value in depth.iteritems()}\n",
    "alpha=np.log(0.5)/np.log(np.median(temp.values()))\n",
    "depth_mod={key:value**alpha for key, value in temp.iteritems()}\n",
    "\n",
    "#temp={key:deg_mod[key]*depth_mod[key] for key in depth_mod.iterkeys()}\n",
    "temp = {}\n",
    "for key in depth_mod.iterkeys():\n",
    "    if key in deg_mod:\n",
    "        temp[key] = deg_mod[key]*depth_mod[key]\n",
    "max_metric=np.max(temp.values())\n",
    "metric={key:value*1./max_metric for key,value in temp.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('max_deg = %s, max_depth = %s',max_deg, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Kmeans\n",
    "NUM_TOPICS = 30\n",
    "K=NUM_TOPICS\n",
    "kmeans=KMeans(n_clusters=K)\n",
    "kmeans.fit([w for w in data_d2v])\n",
    "kmeans_label={word_d2v[x]:kmeans.labels_[x] for x in xrange(len(word_d2v))}\n",
    "\n",
    "kmeans_label_ranked={}\n",
    "\n",
    "topic=[[] for i in xrange(K)]\n",
    "clust_depth=[[] for i in xrange(K)]\n",
    "for i in xrange(K):\n",
    "    topic[i]=[word_d2v[x] for x in xrange(len(word_d2v)) if kmeans.labels_[x]==i]\n",
    "    #temp_score=[metric[w] for w in topic[i]]\n",
    "    temp_score = []\n",
    "    for w in topic[i]:\n",
    "        if w in metric: temp_score.append(metric[w])\n",
    "    clust_depth[i]=-np.mean(sorted(temp_score,reverse=True)[:])#int(np.sqrt(len(topic[i])))])\n",
    "index=np.argsort(clust_depth)\n",
    "for num,i in enumerate(xrange(K)):\n",
    "    for w in topic[index[i]]:\n",
    "        kmeans_label_ranked[w]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Done...Generating output')\n",
    "lister=[]\n",
    "to_show=K\n",
    "to_show_words=20 #the maximum number of words of each type to display\n",
    "for i in xrange(to_show):\n",
    "    top=topic[index[i]]\n",
    "    #sort_top=[w[0] for w in sorted([[w,metric[w]] for w in top],key=itemgetter(1),reverse=True)]\n",
    "    sort_tmp = []\n",
    "    for w in top:\n",
    "        if w in metric: sort_tmp.append([w,metric[w]])\n",
    "    sort_top=[w[0] for w in sorted(sort_tmp,key=itemgetter(1),reverse=True)]\n",
    "    lister.append(['Topic %d' %(i+1)]+sort_top[:to_show_words])\n",
    "\n",
    "max_len=max([len(w) for w in lister])\n",
    "new_list=[]\n",
    "for list_el in lister:\n",
    "    new_list.append(list_el + [''] * (max_len - len(list_el)))\n",
    "Topics=list(itertools.izip_longest(*new_list))\n",
    "#X.insert(len(X),[-int(clust_depth[index[w]]*100)*1./100 for w in xrange(K)])\n",
    "sorted_words=[w[0] for w in sorted(metric.items(),key=itemgetter(1),reverse=True)][:to_show_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_words = sorted_words\n",
    "deep_words = [w[0] for w in depth.most_common(to_show_words)]\n",
    "filer = 'wiki_simple.txt'\n",
    "outfile_topics = data_directory + filer.split('.')[0] + '_topics.csv'\n",
    "outfile_score = data_directory + filer.split('.')[0] + '_score.csv'\n",
    "outfile_depth = data_directory + filer.split('.')[0] + '_depth.csv'\n",
    "b = open(outfile_topics, 'wb')\n",
    "a = csv.writer(b)\n",
    "a.writerows(Topics)\n",
    "b = open(outfile_score, 'wb')\n",
    "a = csv.writer(b)\n",
    "a.writerows([[w] for w in score_words])\n",
    "b = open(outfile_depth, 'wb')\n",
    "a = csv.writer(b)\n",
    "a.writerows([[w] for w in deep_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = 8\n",
    "print 'Total number of Topics = {}.'.format(K)\n",
    "for j in range(K/columns + 1):\n",
    "    first = j*columns + 1;last = (j*columns + columns)\n",
    "    print 'Displaying Topics {} thru {}.'.format(first, last)\n",
    "    print tabulate([Topics[i][first-1:last] for i in range(0,11)], tablefmt=u'psql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
